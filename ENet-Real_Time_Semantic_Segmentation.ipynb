{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UhGVcnMbgr7y"
   },
   "source": [
    "# ENet -  Real Time Semantic Segmentation\n",
    "\n",
    "In this notebook, we have reproduced the ENet paper. <br/>\n",
    "Link to the paper: https://arxiv.org/pdf/1606.02147.pdf <br/>\n",
    "Link to the repository: https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation\n",
    "\n",
    "\n",
    "Star and Fork!\n",
    "\n",
    "\n",
    "**ALL THE CODE IN THIS NOTEBOOK ASSUMES THE USAGE OF THE <span style=\"color:blue;\">CAMVID</span> DATASET**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qnyf_pzhKXv"
   },
   "source": [
    "## Install the dependencies and Import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5NCTHdEqj317"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7m-TI5tHhSka"
   },
   "source": [
    "## Download the CamVid dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 25441
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "VcU98cbVppiu",
    "outputId": "e759b961-9898-4510-f2d0-808db5320f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: https://www.dropbox.com/s/pxcz2wdz04zxocq/CamVid.zip?dl=1\n",
      "unzip:  cannot find or open CamVid.zip, CamVid.zip.zip or CamVid.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/pxcz2wdz04zxocq/CamVid.zip?dl=1 -O CamVid.zip\n",
    "!unzip CamVid.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i26TZVXmhewY"
   },
   "source": [
    "## Create the ENet model\n",
    "\n",
    "We decided to to split the model to three sub classes:\n",
    "\n",
    "1) Initial block  \n",
    "\n",
    "2) RDDNeck - class for regular, downsampling and dilated bottlenecks\n",
    "\n",
    "3) ASNeck -  class for asymetric bottlenecks\n",
    "\n",
    "4) UBNeck - class for upsampling bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqHUezLfPBwn"
   },
   "outputs": [],
   "source": [
    "class InitialBlock(nn.Module):\n",
    "  \n",
    "  # Initial block of the model:\n",
    "  #         Input\n",
    "  #        /     \\\n",
    "  #       /       \\\n",
    "  #maxpool2d    conv2d-3x3\n",
    "  #       \\       /  \n",
    "  #        \\     /\n",
    "  #      concatenate\n",
    "   \n",
    "    def __init__ (self,in_channels = 3,out_channels = 13):\n",
    "        super().__init__()\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, \n",
    "                                      stride = 2, \n",
    "                                      padding = 0)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, \n",
    "                                out_channels,\n",
    "                                kernel_size = 3,\n",
    "                                stride = 2, \n",
    "                                padding = 1)\n",
    "\n",
    "        self.prelu = nn.PReLU(16)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        \n",
    "        main = self.conv(x)\n",
    "        main = self.batchnorm(main)\n",
    "        \n",
    "        side = self.maxpool(x)\n",
    "        \n",
    "        # concatenating on the channels axis\n",
    "        x = torch.cat((main, side), dim=1)\n",
    "        x = self.prelu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERqXRpl_sfSE"
   },
   "outputs": [],
   "source": [
    "class UBNeck(nn.Module):\n",
    "    \n",
    "  # Upsampling bottleneck:\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  # conv2d-1x1     convTrans2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         convTrans2d-3x3\n",
    "  #      |             | PReLU\n",
    "  #      |         convTrans2d-1x1\n",
    "  #      |             |\n",
    "  # maxunpool2d    Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  #  Params: \n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
    "  \n",
    "    def __init__(self, in_channels, out_channels, relu=False, projection_ratio=4):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        self.reduced_depth = int(in_channels / projection_ratio)\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        \n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size = 2,\n",
    "                                     stride = 2)\n",
    "        \n",
    "        self.main_conv = nn.Conv2d(in_channels = self.in_channels,\n",
    "                                    out_channels = self.out_channels,\n",
    "                                    kernel_size = 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        \n",
    "        \n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False)\n",
    "        \n",
    "        \n",
    "        self.prelu1 = activation\n",
    "        \n",
    "        # This layer used for Upsampling\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = 2,\n",
    "                                  padding = 1,\n",
    "                                  output_padding = 1,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu2 = activation\n",
    "        \n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu3 = activation\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "    def forward(self, x, indices):\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.convt1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.convt2(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.convt3(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        \n",
    "        x_copy = self.main_conv(x_copy)\n",
    "        x_copy = self.unpool(x_copy, indices, output_size=x.size())\n",
    "        \n",
    "        # summing the main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-nTIAS9bd9z"
   },
   "outputs": [],
   "source": [
    "class RDDNeck(nn.Module):\n",
    "    def __init__(self, dilation, in_channels, out_channels, down_flag, relu=False, projection_ratio=4, p=0.1):\n",
    "      \n",
    "  # Regular|Dilated|Downsampling bottlenecks:\n",
    "  #\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  # maxpooling2d   conv2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-3x3\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             |\n",
    "  #  Padding2d     Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  # Params: \n",
    "  #  dilation (bool) - if True: creating dilation bottleneck\n",
    "  #  down_flag (bool) - if True: creating downsampling bottleneck\n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
    "  #  p - dropout ratio\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = dilation\n",
    "        self.down_flag = down_flag\n",
    "        \n",
    "        # calculating the number of reduced channels\n",
    "        if down_flag:\n",
    "            self.stride = 2\n",
    "            self.reduced_depth = int(in_channels // projection_ratio)\n",
    "        else:\n",
    "            self.stride = 1\n",
    "            self.reduced_depth = int(out_channels // projection_ratio)\n",
    "        \n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2,\n",
    "                                      stride = 2,\n",
    "                                      padding = 0, return_indices=True)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=p)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False,\n",
    "                               dilation = 1)\n",
    "        \n",
    "        self.prelu1 = activation\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = self.stride,\n",
    "                                  padding = self.dilation,\n",
    "                                  bias = True,\n",
    "                                  dilation = self.dilation)\n",
    "                                  \n",
    "        self.prelu2 = activation\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  stride = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False,\n",
    "                                  dilation = 1)\n",
    "        \n",
    "        self.prelu3 = activation\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        bs = x.size()[0]\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm2(x)\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        if self.down_flag:\n",
    "            x_copy, indices = self.maxpool(x_copy)\n",
    "          \n",
    "        if self.in_channels != self.out_channels:\n",
    "            out_shape = self.out_channels - self.in_channels\n",
    "            \n",
    "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
    "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
    "            if torch.cuda.is_available():\n",
    "                extras = extras.cuda()\n",
    "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
    "\n",
    "        # Summing main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        if self.down_flag:\n",
    "            return x, indices\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tb_i1sCvtmMF"
   },
   "outputs": [],
   "source": [
    "class ASNeck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, projection_ratio=4):\n",
    "      \n",
    "  # Asymetric bottleneck:\n",
    "  #\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x5\n",
    "  #      |             |\n",
    "  #      |         conv2d-5x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             |\n",
    "  #  Padding2d     Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  # Params:    \n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        self.reduced_depth = int(in_channels / projection_ratio)\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False)\n",
    "        \n",
    "        self.prelu1 = nn.PReLU()\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = (1, 5),\n",
    "                                  stride = 1,\n",
    "                                  padding = (0, 2),\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.conv22 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = (5, 1),\n",
    "                                  stride = 1,\n",
    "                                  padding = (2, 0),\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu2 = nn.PReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  stride = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu3 = nn.PReLU()\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.size()[0]\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.conv21(x)\n",
    "        x = self.conv22(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        \n",
    "        if self.in_channels != self.out_channels:\n",
    "            out_shape = self.out_channels - self.in_channels\n",
    "            \n",
    "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
    "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
    "            if torch.cuda.is_available():\n",
    "                extras = extras.cuda()\n",
    "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
    "        \n",
    "        # Summing main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1pz_bve690y"
   },
   "outputs": [],
   "source": [
    "class ENet(nn.Module):\n",
    "  \n",
    "  # Creating Enet model!\n",
    "  \n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "        \n",
    "        # The initial block\n",
    "        self.init = InitialBlock()\n",
    "        \n",
    "        \n",
    "        # The first bottleneck\n",
    "        self.b10 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=64, \n",
    "                           down_flag=True, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b11 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b12 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b13 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b14 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        \n",
    "        # The second bottleneck\n",
    "        self.b20 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=128, \n",
    "                           down_flag=True)\n",
    "        \n",
    "        self.b21 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b22 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b23 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b24 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b25 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b26 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b27 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b28 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The third bottleneck\n",
    "        self.b31 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b32 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b33 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b34 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b35 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b36 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b37 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b38 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        self.b40 = UBNeck(in_channels=128, \n",
    "                          out_channels=64, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b41 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        self.b42 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        self.b50 = UBNeck(in_channels=64, \n",
    "                          out_channels=16, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b51 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=16, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n",
    "                                           out_channels=self.C, \n",
    "                                           kernel_size=3, \n",
    "                                           stride=2, \n",
    "                                           padding=1, \n",
    "                                           output_padding=1,\n",
    "                                           bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # The initial block\n",
    "        x = self.init(x)\n",
    "        \n",
    "        # The first bottleneck\n",
    "        x, i1 = self.b10(x)\n",
    "        x = self.b11(x)\n",
    "        x = self.b12(x)\n",
    "        x = self.b13(x)\n",
    "        x = self.b14(x)\n",
    "        \n",
    "        # The second bottleneck\n",
    "        x, i2 = self.b20(x)\n",
    "        x = self.b21(x)\n",
    "        x = self.b22(x)\n",
    "        x = self.b23(x)\n",
    "        x = self.b24(x)\n",
    "        x = self.b25(x)\n",
    "        x = self.b26(x)\n",
    "        x = self.b27(x)\n",
    "        x = self.b28(x)\n",
    "        \n",
    "        # The third bottleneck\n",
    "        x = self.b31(x)\n",
    "        x = self.b32(x)\n",
    "        x = self.b33(x)\n",
    "        x = self.b34(x)\n",
    "        x = self.b35(x)\n",
    "        x = self.b36(x)\n",
    "        x = self.b37(x)\n",
    "        x = self.b38(x)\n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        x = self.b40(x, i2)\n",
    "        x = self.b41(x)\n",
    "        x = self.b42(x)\n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        x = self.b50(x, i1)\n",
    "        x = self.b51(x)\n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        x = self.fullconv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jg1Rnb3uhnxR"
   },
   "source": [
    "## Instantiate the ENet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3UdSwlKiB27K"
   },
   "outputs": [],
   "source": [
    "enet = ENet(12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W5kaK6CnhwG_"
   },
   "source": [
    "Move the model to cuda if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZcgE-F_hvxX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Checking if there is any gpu available and pass the model to  or cpu\n",
    "\n",
    "# Set the device      \n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "enet = enet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvURoRuSlyvP"
   },
   "source": [
    "## Define the loader that will load the input and output images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFCIsDF2bpQx"
   },
   "outputs": [],
   "source": [
    "def loader(training_path, segmented_path, batch_size, h=512, w=512):\n",
    "    #print(training_path)\n",
    "    filenames_t = glob.glob(training_path+f'*/*.jpg')\n",
    "    #print(filenames_t)\n",
    "    total_files_t = len(filenames_t)\n",
    "\n",
    "    filenames_s = glob.glob(segmented_path+f'*/*.png') \n",
    "    total_files_s = len(filenames_s)#contain only png file names\n",
    "    \n",
    "    #print(filenames_s)\n",
    "    print(total_files_s, total_files_t)\n",
    "    assert(total_files_t == total_files_s)\n",
    "    \n",
    "    if str(batch_size).lower() == 'all':\n",
    "        batch_size = 500\n",
    "    \n",
    "    print(batch_size)\n",
    "    idx = 0\n",
    "    while(1):\n",
    "        print('inside loader')\n",
    "        batch_idxs = np.random.randint(0, total_files_s, batch_size)\n",
    "            \n",
    "        #print(len(batch_idxs))\n",
    "        inputs = []\n",
    "        labels = []\n",
    "        \n",
    "\n",
    "        for jj in tqdm(batch_idxs):\n",
    "            #print(jj)\n",
    "            img = plt.imread(filenames_t[jj])\n",
    "            #print(filenames_t[jj])\n",
    "            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n",
    "            inputs.append(img)\n",
    "            \n",
    "            img = Image.open(filenames_s[jj])\n",
    "            img = np.array(img)\n",
    "            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n",
    "            labels.append(img)\n",
    "        \n",
    "        #print(labels)\n",
    "        #print(len(inputs))\n",
    "        #print(len(labels))\n",
    "        inputs = np.stack(inputs, axis=2)\n",
    "        inputs = torch.tensor(inputs).transpose(0, 2).transpose(1, 3)\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        yield inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_4qD8VBah2K2"
   },
   "source": [
    "## Define the class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GmnNEeMHiSU2"
   },
   "outputs": [],
   "source": [
    "def get_class_weights(num_classes, c=1.02):\n",
    "    \n",
    "    pipe = loader('/Users/dalex/final_year_proj/datasets/IDD/idd20kII/leftImg8bit/train/', '/Users/dalex/final_year_proj/datasets/IDD/idd20kII/gtFine/train/', batch_size='all')\n",
    "    _, labels = next(pipe)\n",
    "    all_labels = labels.flatten()\n",
    "    print(all_labels.shape)\n",
    "    each_class = np.bincount(all_labels, minlength=num_classes)\n",
    "    print(each_class)\n",
    "    prospensity_score = each_class / len(all_labels)\n",
    "    class_weights = 1 / (np.log(c + prospensity_score))\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHBoLmadmrA_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7034 7034\n",
      "500\n",
      "inside loader\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:15<00:00, 33.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([131072000])\n",
      "[34374293  8175478   545910  2035466   821259   866554  1279364    68985\n",
      "  1636835  2420327  1557237   620176   513527  2026815  2244960   665010\n",
      "   269556  1594952   155914    57618  1461143  5392183  9014418  1611744\n",
      " 22252382 29237256       71       43       48       53       63       38\n",
      "       53       64       46       63       53       67       62       42\n",
      "       74       74       58       39       62       61       56       53\n",
      "       36      119       91      136       46      167       21       43\n",
      "       20       29       32       31       35       35       33       34\n",
      "       26       19       41       26       27       26       47       41\n",
      "       16       33       33       25       36       21       46       32\n",
      "       39       37       25       22       36       30     1283       42\n",
      "       28       39       22       32       23       19       49       20\n",
      "       17       39       43       27       30       35       55       44\n",
      "       30       22       31       39      207      109       40      163\n",
      "       17       16     7197      203       29       18       41       61\n",
      "       40       29       45       27       11       83      355     3878\n",
      "       19       45       37       18       31       48       31       12\n",
      "       37       35       25       17       17       45       23       25\n",
      "       28       40       16       25       16       35       20       22\n",
      "       20       14       26       27       32       14       30       79\n",
      "       28       76       26       41       38       39       30      304\n",
      "      285       85       52       28       16       26       31       21\n",
      "       15       37       18       10       25       46       17       13\n",
      "       27       22       26       17       36       38       22       38\n",
      "       18       35       14       32       24       12       10       44\n",
      "       15       21       21       36       16       29       43       20\n",
      "       26       26       33        5       35       39        8       20\n",
      "       18       44       20       12       55       21       12       22\n",
      "       94       27      346       29       60       19       19       49\n",
      "       35       26       33       48       15       18       32       49\n",
      "       13       23       58       21       20       18       48       25\n",
      "       12       27       56       16       21       68       15   150697]\n",
      "[ 4.02219802 12.63316492 41.88023826 28.6428166  38.57029367 38.0758577\n",
      " 34.09883425 49.21626029 31.27787482 26.49408942 31.86425042 40.93211796\n",
      " 42.30772082 28.69520473 27.43104073 40.38048436 45.83619002 31.58365523\n",
      " 47.69137245 49.42297323 32.6026088  16.85120257 11.7574026  31.46033321\n",
      "  5.7550028   4.59604991 50.49699557 50.49752962 50.49743425 50.49733888\n",
      " 50.49714815 50.49762499 50.49733888 50.49712908 50.4974724  50.49714815\n",
      " 50.49733888 50.49707186 50.49716723 50.49754869 50.49693835 50.49693835\n",
      " 50.49724352 50.49760591 50.49716723 50.4971863  50.49728166 50.49733888\n",
      " 50.49766313 50.49608008 50.49661411 50.49575585 50.4974724  50.49516462\n",
      " 50.49794924 50.49752962 50.49796831 50.49779665 50.49773943 50.4977585\n",
      " 50.49768221 50.49768221 50.49772035 50.49770128 50.49785387 50.49798739\n",
      " 50.49756777 50.49785387 50.4978348  50.49785387 50.49745332 50.49756777\n",
      " 50.49804461 50.49772035 50.49772035 50.49787294 50.49766313 50.49794924\n",
      " 50.4974724  50.49773943 50.49760591 50.49764406 50.49787294 50.49793016\n",
      " 50.49766313 50.49777758 50.47388968 50.49754869 50.49781572 50.49760591\n",
      " 50.49793016 50.49773943 50.49791109 50.49798739 50.49741518 50.49796831\n",
      " 50.49802553 50.49760591 50.49752962 50.4978348  50.49777758 50.49768221\n",
      " 50.49730074 50.49751055 50.49777758 50.49793016 50.4977585  50.49760591\n",
      " 50.49440176 50.4962708  50.49758684 50.49524091 50.49802553 50.49804461\n",
      " 50.36144926 50.49447805 50.49779665 50.49800646 50.49756777 50.4971863\n",
      " 50.49758684 50.49779665 50.49749147 50.4978348  50.49813998 50.49676669\n",
      " 50.4915794  50.42448966 50.49798739 50.49749147 50.49764406 50.49800646\n",
      " 50.4977585  50.49743425 50.4977585  50.4981209  50.49764406 50.49768221\n",
      " 50.49787294 50.49802553 50.49802553 50.49749147 50.49791109 50.49787294\n",
      " 50.49781572 50.49758684 50.49804461 50.49787294 50.49804461 50.49768221\n",
      " 50.49796831 50.49793016 50.49796831 50.49808276 50.49785387 50.4978348\n",
      " 50.49773943 50.49808276 50.49777758 50.49684298 50.49781572 50.4969002\n",
      " 50.49785387 50.49756777 50.49762499 50.49760591 50.49777758 50.49255193\n",
      " 50.49291426 50.49672855 50.49735796 50.49781572 50.49804461 50.49785387\n",
      " 50.4977585  50.49794924 50.49806368 50.49764406 50.49800646 50.49815905\n",
      " 50.49787294 50.4974724  50.49802553 50.49810183 50.4978348  50.49793016\n",
      " 50.49785387 50.49802553 50.49766313 50.49762499 50.49793016 50.49762499\n",
      " 50.49800646 50.49768221 50.49808276 50.49773943 50.49789202 50.4981209\n",
      " 50.49815905 50.49751055 50.49806368 50.49794924 50.49794924 50.49766313\n",
      " 50.49804461 50.49779665 50.49752962 50.49796831 50.49785387 50.49785387\n",
      " 50.49772035 50.49825442 50.49768221 50.49760591 50.4981972  50.49796831\n",
      " 50.49800646 50.49751055 50.49796831 50.4981209  50.49730074 50.49794924\n",
      " 50.4981209  50.49793016 50.49655689 50.4978348  50.49175102 50.49779665\n",
      " 50.49720537 50.49798739 50.49798739 50.49741518 50.49768221 50.49785387\n",
      " 50.49772035 50.49743425 50.49806368 50.49800646 50.49773943 50.49741518\n",
      " 50.49810183 50.49791109 50.49724352 50.49794924 50.49796831 50.49800646\n",
      " 50.49743425 50.49787294 50.4981209  50.4978348  50.49728166 50.49804461\n",
      " 50.49794924 50.49705279 50.49806368 47.78019039]\n"
     ]
    }
   ],
   "source": [
    "class_weights = get_class_weights(30)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wwnNuFIfhsXm"
   },
   "source": [
    "## Define the Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DI9425iz7thP"
   },
   "outputs": [],
   "source": [
    "lr = 5e-4\n",
    "batch_size = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
    "optimizer = torch.optim.Adam(enet.parameters(), \n",
    "                             lr=lr,\n",
    "                             weight_decay=2e-4)\n",
    "\n",
    "print_every = 5\n",
    "eval_every = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFIdlVWviBYl"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "WQ6XJzl6Ta1_",
    "outputId": "a3f62522-391d-4e05-f138-11c78a0d90cf"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "\n",
    "bc_train = 367 // batch_size # mini_batch train\n",
    "bc_eval = 101 // batch_size  # mini_batch validation\n",
    "\n",
    "# Define pipeline objects\n",
    "pipe = loader('/content/train/', '/content/trainannot/', batch_size)\n",
    "eval_pipe = loader('/content/val/', '/content/valannot/', batch_size)\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "# Train loop\n",
    "\n",
    "for e in range(1, epochs+1):\n",
    "    \n",
    "    \n",
    "    train_loss = 0\n",
    "    print ('-'*15,'Epoch %d' % e, '-'*15)\n",
    "    \n",
    "    enet.train()\n",
    "    \n",
    "    for _ in tqdm(range(bc_train)):\n",
    "        X_batch, mask_batch = next(pipe)\n",
    "        \n",
    "        # assign data to cpu/gpu\n",
    "        X_batch, mask_batch = X_batch.to(device), mask_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = enet(X_batch.float())\n",
    "        \n",
    "        # loss calculation\n",
    "        loss = criterion(out, mask_batch.long())\n",
    "        # update weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        \n",
    "    print ()\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    if (e+1) % print_every == 0:\n",
    "        print ('Epoch {}/{}...'.format(e, epochs),\n",
    "                'Loss {:6f}'.format(train_loss))\n",
    "    \n",
    "    if e % eval_every == 0:\n",
    "        with torch.no_grad():\n",
    "            enet.eval()\n",
    "            \n",
    "            eval_loss = 0\n",
    "\n",
    "            # Validation loop\n",
    "            for _ in tqdm(range(bc_eval)):\n",
    "                inputs, labels = next(eval_pipe)\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    \n",
    "                \n",
    "                out = enet(inputs)\n",
    "                \n",
    "                out = out.data.max(1)[1]\n",
    "                \n",
    "                eval_loss += (labels.long() - out.long()).sum()\n",
    "                \n",
    "            \n",
    "            print ()\n",
    "            print ('Loss {:6f}'.format(eval_loss))\n",
    "            \n",
    "            eval_losses.append(eval_loss)\n",
    "        \n",
    "    if e % print_every == 0:\n",
    "        checkpoint = {\n",
    "            'epochs' : e,\n",
    "            'state_dict' : enet.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, '/content/ckpt-enet-{}-{}.pth'.format(e, train_loss))\n",
    "        print ('Model saved!')\n",
    "\n",
    "print ('Epoch {}/{}...'.format(e, epochs),\n",
    "       'Total Mean Loss: {:6f}'.format(sum(train_losses) / epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bdp5YLb0ibFO"
   },
   "source": [
    "## Infer using the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cspEQPgEKVvg"
   },
   "source": [
    "Get the PreTrained ENet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "kbxiHfeVKGg_",
    "outputId": "fd20e12b-7b64-4539-b2e2-91c06fa452b9"
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/iArunava/ENet-Real-Time-Semantic-Segmentation/raw/master/datasets/CamVid/ckpt-enet.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9CV4mHhKYZ5"
   },
   "source": [
    "Load the ENet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXB9XRE3iddC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pretrained model if needed\n",
    "\n",
    "from models.ENet import ENet\n",
    "enet = ENet(12)\n",
    "state_dict = torch.load('/Users/dalex/datascience/ENet-Real-Time-Semantic-Segmentation/ckpt-enet-100-18.116573244333267.pth')['state_dict']\n",
    "enet.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dWM6D5Rl_Mq"
   },
   "source": [
    "## Use the code to infer on new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8pxajTHj5vNG"
   },
   "outputs": [],
   "source": [
    "fname = 'Seq05VD_f05100.png'\n",
    "tmg_ = plt.imread('/content/test/' + fname)\n",
    "tmg_ = cv2.resize(tmg_, (512, 512), cv2.INTER_NEAREST)\n",
    "tmg = torch.tensor(tmg_).unsqueeze(0).float()\n",
    "tmg = tmg.transpose(2, 3).transpose(1, 2).to(device)\n",
    "\n",
    "enet.to(device)\n",
    "with torch.no_grad():\n",
    "    out1 = enet(tmg.float()).squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8vh2eozpAAh"
   },
   "source": [
    "## Load the label image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zQiSaeeo_rZ"
   },
   "outputs": [],
   "source": [
    "smg_ = Image.open('/content/testannot/' + fname)\n",
    "smg_ = cv2.resize(np.array(smg_), (512, 512), cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EXLLTnYpF2D"
   },
   "source": [
    "## Move the output to cpu and convert it to numpy and see how each class looks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjpnkPB6RKub"
   },
   "outputs": [],
   "source": [
    "out2 = out1.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "U-sVZCMU6vgF",
    "outputId": "5d9c186e-75e2-4414-c6e3-b788453dbad7"
   },
   "outputs": [],
   "source": [
    "mno = 8 # Should be between 0 - n-1 | where n is the number of classes\n",
    "\n",
    "figure = plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Input Image')\n",
    "plt.axis('off')\n",
    "plt.imshow(tmg_)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Output Image')\n",
    "plt.axis('off')\n",
    "plt.imshow(out2[mno, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8yIfMvrpZkT"
   },
   "source": [
    "Get the class labels from the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e1C9bmgYxkga"
   },
   "outputs": [],
   "source": [
    "b_ = out1.data.max(0)[1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLw_Y8emp9HM"
   },
   "source": [
    "Define the function that maps a 2D image with all the class labels to a segmented image with the specified colored maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkyoipIGZKk9"
   },
   "outputs": [],
   "source": [
    "def decode_segmap(image):\n",
    "    Sky = [128, 128, 128]\n",
    "    Building = [128, 0, 0]\n",
    "    Pole = [192, 192, 128]\n",
    "    Road_marking = [255, 69, 0]\n",
    "    Road = [128, 64, 128]\n",
    "    Pavement = [60, 40, 222]\n",
    "    Tree = [128, 128, 0]\n",
    "    SignSymbol = [192, 128, 128]\n",
    "    Fence = [64, 64, 128]\n",
    "    Car = [64, 0, 128]\n",
    "    Pedestrian = [64, 64, 0]\n",
    "    Bicyclist = [0, 128, 192]\n",
    "\n",
    "    label_colours = np.array([Sky, Building, Pole, Road_marking, Road, \n",
    "                              Pavement, Tree, SignSymbol, Fence, Car, \n",
    "                              Pedestrian, Bicyclist]).astype(np.uint8)\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    for l in range(0, 12):\n",
    "        r[image == l] = label_colours[l, 0]\n",
    "        g[image == l] = label_colours[l, 1]\n",
    "        b[image == l] = label_colours[l, 2]\n",
    "\n",
    "    rgb = np.zeros((image.shape[0], image.shape[1], 3)).astype(np.uint8)\n",
    "    rgb[:, :, 0] = b\n",
    "    rgb[:, :, 1] = g\n",
    "    rgb[:, :, 2] = r\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_B6n9VmqP5g"
   },
   "source": [
    "Decode the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHyCu5TTaNkv"
   },
   "outputs": [],
   "source": [
    "true_seg = decode_segmap(smg_)\n",
    "pred_seg = decode_segmap(b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "4G_8C40YXziZ",
    "outputId": "75ed8f50-c135-4eb8-9b5c-1a63e2620991"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Input Image')\n",
    "plt.axis('off')\n",
    "plt.imshow(tmg_)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Predicted Segmentation')\n",
    "plt.axis('off')\n",
    "plt.imshow(pred_seg)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Ground Truth')\n",
    "plt.axis('off')\n",
    "plt.imshow(true_seg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YI65NDYmr7h"
   },
   "source": [
    "## Save the model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WjUZDGfU5F8X"
   },
   "outputs": [],
   "source": [
    "# Save the parameters\n",
    "checkpoint = {\n",
    "    'epochs' : e,\n",
    "    'state_dict' : enet.state_dict()\n",
    "}\n",
    "torch.save(checkpoint, 'ckpt-enet-1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51UgRcNYmwCc"
   },
   "source": [
    "## Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQVVoXhn5oun"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(enet, '/content/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intersection_over_union(pred_mask, true_mask):\n",
    "    intersection = torch.logical_and(pred_mask, true_mask).sum().item()\n",
    "    union = torch.logical_or(pred_mask, true_mask).sum().item()\n",
    "\n",
    "    iou = intersection / union if union > 0 else 0.0\n",
    "    return iou\n",
    "\n",
    "def mean_iou(predictions, targets, num_classes):\n",
    "    class_iou = [0] * num_classes\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        pred_mask = (predictions == class_idx)\n",
    "        true_mask = (targets == class_idx)\n",
    "        class_iou[class_idx] = intersection_over_union(pred_mask, true_mask)\n",
    "\n",
    "    mean_iou_value = sum(class_iou) / num_classes\n",
    "    \n",
    "    print('road iou',class_iou[4])\n",
    "    return class_iou[4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n",
      "(512, 512, 3)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (3) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m load_validation_set(validation_path, segmentation_path)\n\u001b[0;32m---> 67\u001b[0m mIoU_eval \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_mIoU\u001b[49m\u001b[43m(\u001b[49m\u001b[43menet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroad IoU on the KITTI dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mIoU_eval)\n",
      "Cell \u001b[0;32mIn[31], line 51\u001b[0m, in \u001b[0;36mcalculate_mIoU\u001b[0;34m(model, inputs, labels, device, num_classes)\u001b[0m\n\u001b[1;32m     48\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     49\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m     batch_iou \u001b[38;5;241m=\u001b[39m \u001b[43mmean_iou\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     mIoU \u001b[38;5;241m=\u001b[39m batch_iou\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mIoU\n",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m, in \u001b[0;36mmean_iou\u001b[0;34m(predictions, targets, num_classes)\u001b[0m\n\u001b[1;32m     12\u001b[0m     pred_mask \u001b[38;5;241m=\u001b[39m (predictions \u001b[38;5;241m==\u001b[39m class_idx)\n\u001b[1;32m     13\u001b[0m     true_mask \u001b[38;5;241m=\u001b[39m (targets \u001b[38;5;241m==\u001b[39m class_idx)\n\u001b[0;32m---> 14\u001b[0m     class_iou[class_idx] \u001b[38;5;241m=\u001b[39m \u001b[43mintersection_over_union\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m mean_iou_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(class_iou) \u001b[38;5;241m/\u001b[39m num_classes\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroad iou\u001b[39m\u001b[38;5;124m'\u001b[39m,class_iou[\u001b[38;5;241m4\u001b[39m])\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mintersection_over_union\u001b[0;34m(pred_mask, true_mask)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mintersection_over_union\u001b[39m(pred_mask, true_mask):\n\u001b[0;32m----> 2\u001b[0m     intersection \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogical_and\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      3\u001b[0m     union \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogical_or(pred_mask, true_mask)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m      5\u001b[0m     iou \u001b[38;5;241m=\u001b[39m intersection \u001b[38;5;241m/\u001b[39m union \u001b[38;5;28;01mif\u001b[39;00m union \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (3) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "def load_validation_set(validation_path, segmentation_path, h=512, w=512):\n",
    "    filenames = os.listdir(validation_path)\n",
    "    total_files = len(filenames)\n",
    "    anotnames = os.listdir(segmentation_path)\n",
    "\n",
    "    inputs = []\n",
    "    labels = []\n",
    "\n",
    "    for jj in range(total_files):\n",
    "\n",
    "        if 'lane' in filenames[jj]:\n",
    "            continue\n",
    "\n",
    "        # Read normalized photo\n",
    "        img = plt.imread(validation_path + filenames[jj])\n",
    "        img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n",
    "        #print(img.shape)\n",
    "        inputs.append(img)\n",
    "\n",
    "        # Read semantic image\n",
    "\n",
    "        img = Image.open(segmentation_path + anotnames[jj])\n",
    "        img = np.array(img)\n",
    "        img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n",
    "        labels.append(img)\n",
    "        #print(img.shape)\n",
    "\n",
    "    inputs = np.stack(inputs, axis=2)\n",
    "    inputs = torch.tensor(inputs).transpose(0, 2).transpose(1, 3)\n",
    "\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return inputs, labels\n",
    "\n",
    "def calculate_mIoU(model, inputs, labels, device, num_classes):\n",
    "    model.eval()\n",
    "\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        batch_iou = mean_iou(predictions, labels, num_classes)\n",
    "        mIoU = batch_iou\n",
    "    return mIoU\n",
    "\n",
    "# Example usage:\n",
    "# Assuming validation_path is the path to your validation dataset\n",
    "# segmentation_path is the path to your validation segmentation annotations\n",
    "# model is your trained segmentation model\n",
    "# num_classes is the number of classes in your segmentation task\n",
    "\n",
    "validation_path = '/Users/dalex/Documents/college/M2/mini project/data_road(DONT_EDIT)/training/image_2/'\n",
    "segmentation_path = '/Users/dalex/Documents/college/M2/mini project/data_road(DONT_EDIT)/training/gt_image_2/'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "inputs, labels = load_validation_set(validation_path, segmentation_path)\n",
    "mIoU_eval = calculate_mIoU(enet, inputs, labels, device, 12)\n",
    "print(\"road IoU on the KITTI dataset:\", mIoU_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m load_validation_set(validation_path, segmentation_path)\n\u001b[0;32m----> 7\u001b[0m mIoU_eval \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_mIoU\u001b[49m\u001b[43m(\u001b[49m\u001b[43menet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroad IoU on the test dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m, mIoU_eval)\n",
      "Cell \u001b[0;32mIn[9], line 44\u001b[0m, in \u001b[0;36mcalculate_mIoU\u001b[0;34m(model, inputs, labels, device, num_classes)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     43\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m---> 44\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     batch_iou \u001b[38;5;241m=\u001b[39m mean_iou(predictions, labels, num_classes)\n\u001b[1;32m     47\u001b[0m     mIoU \u001b[38;5;241m=\u001b[39m batch_iou\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation_path = '/Users/dalex/Documents/college/M2/mini project/THURS_MINI_PRO/data_road/training/image_2/'\n",
    "segmentation_path = '/Users/dalex/Documents/college/M2/mini project/THURS_MINI_PRO/data_road/training/gt_image_2/'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "inputs, labels = load_validation_set(validation_path, segmentation_path)\n",
    "mIoU_eval = calculate_mIoU(enet, inputs, labels, device, 12)\n",
    "print(\"road IoU on the test dataset:\", mIoU_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in /Users/dalex/datascience/env/lib/python3.8/site-packages (1.5.1)\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 13, 256, 256]             364\n",
      "       BatchNorm2d-2         [-1, 13, 256, 256]              26\n",
      "         MaxPool2d-3          [-1, 3, 256, 256]               0\n",
      "             PReLU-4         [-1, 16, 256, 256]              16\n",
      "      InitialBlock-5         [-1, 16, 256, 256]               0\n",
      "            Conv2d-6          [-1, 4, 256, 256]              64\n",
      "       BatchNorm2d-7          [-1, 4, 256, 256]               8\n",
      "             PReLU-8          [-1, 4, 256, 256]               1\n",
      "            Conv2d-9          [-1, 4, 128, 128]             148\n",
      "      BatchNorm2d-10          [-1, 4, 128, 128]               8\n",
      "            PReLU-11          [-1, 4, 128, 128]               1\n",
      "           Conv2d-12         [-1, 64, 128, 128]             256\n",
      "      BatchNorm2d-13         [-1, 64, 128, 128]             128\n",
      "        Dropout2d-14         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-15  [[-1, 16, 128, 128], [-1, 16, 128, 128]]               0\n",
      "            PReLU-16         [-1, 64, 128, 128]               1\n",
      "          RDDNeck-17  [[-1, 64, 128, 128], [-1, 16, 128, 128]]               0\n",
      "           Conv2d-18         [-1, 16, 128, 128]           1,024\n",
      "      BatchNorm2d-19         [-1, 16, 128, 128]              32\n",
      "            PReLU-20         [-1, 16, 128, 128]               1\n",
      "           Conv2d-21         [-1, 16, 128, 128]           2,320\n",
      "      BatchNorm2d-22         [-1, 16, 128, 128]              32\n",
      "            PReLU-23         [-1, 16, 128, 128]               1\n",
      "           Conv2d-24         [-1, 64, 128, 128]           1,024\n",
      "      BatchNorm2d-25         [-1, 64, 128, 128]             128\n",
      "        Dropout2d-26         [-1, 64, 128, 128]               0\n",
      "            PReLU-27         [-1, 64, 128, 128]               1\n",
      "          RDDNeck-28         [-1, 64, 128, 128]               0\n",
      "           Conv2d-29         [-1, 16, 128, 128]           1,024\n",
      "      BatchNorm2d-30         [-1, 16, 128, 128]              32\n",
      "            PReLU-31         [-1, 16, 128, 128]               1\n",
      "           Conv2d-32         [-1, 16, 128, 128]           2,320\n",
      "      BatchNorm2d-33         [-1, 16, 128, 128]              32\n",
      "            PReLU-34         [-1, 16, 128, 128]               1\n",
      "           Conv2d-35         [-1, 64, 128, 128]           1,024\n",
      "      BatchNorm2d-36         [-1, 64, 128, 128]             128\n",
      "        Dropout2d-37         [-1, 64, 128, 128]               0\n",
      "            PReLU-38         [-1, 64, 128, 128]               1\n",
      "          RDDNeck-39         [-1, 64, 128, 128]               0\n",
      "           Conv2d-40         [-1, 16, 128, 128]           1,024\n",
      "      BatchNorm2d-41         [-1, 16, 128, 128]              32\n",
      "            PReLU-42         [-1, 16, 128, 128]               1\n",
      "           Conv2d-43         [-1, 16, 128, 128]           2,320\n",
      "      BatchNorm2d-44         [-1, 16, 128, 128]              32\n",
      "            PReLU-45         [-1, 16, 128, 128]               1\n",
      "           Conv2d-46         [-1, 64, 128, 128]           1,024\n",
      "      BatchNorm2d-47         [-1, 64, 128, 128]             128\n",
      "        Dropout2d-48         [-1, 64, 128, 128]               0\n",
      "            PReLU-49         [-1, 64, 128, 128]               1\n",
      "          RDDNeck-50         [-1, 64, 128, 128]               0\n",
      "           Conv2d-51         [-1, 16, 128, 128]           1,024\n",
      "      BatchNorm2d-52         [-1, 16, 128, 128]              32\n",
      "            PReLU-53         [-1, 16, 128, 128]               1\n",
      "           Conv2d-54         [-1, 16, 128, 128]           2,320\n",
      "      BatchNorm2d-55         [-1, 16, 128, 128]              32\n",
      "            PReLU-56         [-1, 16, 128, 128]               1\n",
      "           Conv2d-57         [-1, 64, 128, 128]           1,024\n",
      "      BatchNorm2d-58         [-1, 64, 128, 128]             128\n",
      "        Dropout2d-59         [-1, 64, 128, 128]               0\n",
      "            PReLU-60         [-1, 64, 128, 128]               1\n",
      "          RDDNeck-61         [-1, 64, 128, 128]               0\n",
      "           Conv2d-62         [-1, 16, 128, 128]           1,024\n",
      "      BatchNorm2d-63         [-1, 16, 128, 128]              32\n",
      "            PReLU-64         [-1, 16, 128, 128]               1\n",
      "           Conv2d-65           [-1, 16, 64, 64]           2,320\n",
      "      BatchNorm2d-66           [-1, 16, 64, 64]              32\n",
      "            PReLU-67           [-1, 16, 64, 64]               1\n",
      "           Conv2d-68          [-1, 128, 64, 64]           2,048\n",
      "      BatchNorm2d-69          [-1, 128, 64, 64]             256\n",
      "        Dropout2d-70          [-1, 128, 64, 64]               0\n",
      "        MaxPool2d-71  [[-1, 64, 64, 64], [-1, 64, 64, 64]]               0\n",
      "            PReLU-72          [-1, 128, 64, 64]               1\n",
      "          RDDNeck-73  [[-1, 128, 64, 64], [-1, 64, 64, 64]]               0\n",
      "           Conv2d-74           [-1, 32, 64, 64]           4,096\n",
      "      BatchNorm2d-75           [-1, 32, 64, 64]              64\n",
      "            PReLU-76           [-1, 32, 64, 64]               1\n",
      "           Conv2d-77           [-1, 32, 64, 64]           9,248\n",
      "      BatchNorm2d-78           [-1, 32, 64, 64]              64\n",
      "            PReLU-79           [-1, 32, 64, 64]               1\n",
      "           Conv2d-80          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-81          [-1, 128, 64, 64]             256\n",
      "        Dropout2d-82          [-1, 128, 64, 64]               0\n",
      "            PReLU-83          [-1, 128, 64, 64]               1\n",
      "          RDDNeck-84          [-1, 128, 64, 64]               0\n",
      "           Conv2d-85           [-1, 32, 64, 64]           4,096\n",
      "      BatchNorm2d-86           [-1, 32, 64, 64]              64\n",
      "            PReLU-87           [-1, 32, 64, 64]               1\n",
      "           Conv2d-88           [-1, 32, 64, 64]           9,248\n",
      "      BatchNorm2d-89           [-1, 32, 64, 64]              64\n",
      "            PReLU-90           [-1, 32, 64, 64]               1\n",
      "           Conv2d-91          [-1, 128, 64, 64]           4,096\n",
      "      BatchNorm2d-92          [-1, 128, 64, 64]             256\n",
      "        Dropout2d-93          [-1, 128, 64, 64]               0\n",
      "            PReLU-94          [-1, 128, 64, 64]               1\n",
      "          RDDNeck-95          [-1, 128, 64, 64]               0\n",
      "           Conv2d-96           [-1, 32, 64, 64]           4,096\n",
      "      BatchNorm2d-97           [-1, 32, 64, 64]              64\n",
      "            PReLU-98           [-1, 32, 64, 64]               1\n",
      "           Conv2d-99           [-1, 32, 64, 64]           5,120\n",
      "          Conv2d-100           [-1, 32, 64, 64]           5,120\n",
      "     BatchNorm2d-101           [-1, 32, 64, 64]              64\n",
      "           PReLU-102           [-1, 32, 64, 64]               1\n",
      "          Conv2d-103          [-1, 128, 64, 64]           4,096\n",
      "       Dropout2d-104          [-1, 128, 64, 64]               0\n",
      "     BatchNorm2d-105          [-1, 128, 64, 64]             256\n",
      "           PReLU-106          [-1, 128, 64, 64]               1\n",
      "          ASNeck-107          [-1, 128, 64, 64]               0\n",
      "          Conv2d-108           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-109           [-1, 32, 64, 64]              64\n",
      "           PReLU-110           [-1, 32, 64, 64]               1\n",
      "          Conv2d-111           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-112           [-1, 32, 64, 64]              64\n",
      "           PReLU-113           [-1, 32, 64, 64]               1\n",
      "          Conv2d-114          [-1, 128, 64, 64]           4,096\n",
      "     BatchNorm2d-115          [-1, 128, 64, 64]             256\n",
      "       Dropout2d-116          [-1, 128, 64, 64]               0\n",
      "           PReLU-117          [-1, 128, 64, 64]               1\n",
      "         RDDNeck-118          [-1, 128, 64, 64]               0\n",
      "          Conv2d-119           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-120           [-1, 32, 64, 64]              64\n",
      "           PReLU-121           [-1, 32, 64, 64]               1\n",
      "          Conv2d-122           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-123           [-1, 32, 64, 64]              64\n",
      "           PReLU-124           [-1, 32, 64, 64]               1\n",
      "          Conv2d-125          [-1, 128, 64, 64]           4,096\n",
      "     BatchNorm2d-126          [-1, 128, 64, 64]             256\n",
      "       Dropout2d-127          [-1, 128, 64, 64]               0\n",
      "           PReLU-128          [-1, 128, 64, 64]               1\n",
      "         RDDNeck-129          [-1, 128, 64, 64]               0\n",
      "          Conv2d-130           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-131           [-1, 32, 64, 64]              64\n",
      "           PReLU-132           [-1, 32, 64, 64]               1\n",
      "          Conv2d-133           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-134           [-1, 32, 64, 64]              64\n",
      "           PReLU-135           [-1, 32, 64, 64]               1\n",
      "          Conv2d-136          [-1, 128, 64, 64]           4,096\n",
      "     BatchNorm2d-137          [-1, 128, 64, 64]             256\n",
      "       Dropout2d-138          [-1, 128, 64, 64]               0\n",
      "           PReLU-139          [-1, 128, 64, 64]               1\n",
      "         RDDNeck-140          [-1, 128, 64, 64]               0\n",
      "          Conv2d-141           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-142           [-1, 32, 64, 64]              64\n",
      "           PReLU-143           [-1, 32, 64, 64]               1\n",
      "          Conv2d-144           [-1, 32, 64, 64]           5,120\n",
      "          Conv2d-145           [-1, 32, 64, 64]           5,120\n",
      "     BatchNorm2d-146           [-1, 32, 64, 64]              64\n",
      "           PReLU-147           [-1, 32, 64, 64]               1\n",
      "          Conv2d-148          [-1, 128, 64, 64]           4,096\n",
      "       Dropout2d-149          [-1, 128, 64, 64]               0\n",
      "     BatchNorm2d-150          [-1, 128, 64, 64]             256\n",
      "           PReLU-151          [-1, 128, 64, 64]               1\n",
      "          ASNeck-152          [-1, 128, 64, 64]               0\n",
      "          Conv2d-153           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-154           [-1, 32, 64, 64]              64\n",
      "           PReLU-155           [-1, 32, 64, 64]               1\n",
      "          Conv2d-156           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-157           [-1, 32, 64, 64]              64\n",
      "           PReLU-158           [-1, 32, 64, 64]               1\n",
      "          Conv2d-159          [-1, 128, 64, 64]           4,096\n",
      "     BatchNorm2d-160          [-1, 128, 64, 64]             256\n",
      "       Dropout2d-161          [-1, 128, 64, 64]               0\n",
      "           PReLU-162          [-1, 128, 64, 64]               1\n",
      "         RDDNeck-163          [-1, 128, 64, 64]               0\n",
      "          Conv2d-164           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-165           [-1, 32, 64, 64]              64\n",
      "           PReLU-166           [-1, 32, 64, 64]               1\n",
      "          Conv2d-167           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-168           [-1, 32, 64, 64]              64\n",
      "           PReLU-169           [-1, 32, 64, 64]               1\n",
      "          Conv2d-170          [-1, 128, 64, 64]           4,096\n",
      "     BatchNorm2d-171          [-1, 128, 64, 64]             256\n",
      "       Dropout2d-172          [-1, 128, 64, 64]               0\n",
      "           PReLU-173          [-1, 128, 64, 64]               1\n",
      "         RDDNeck-174          [-1, 128, 64, 64]               0\n",
      "          Conv2d-175           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-176           [-1, 32, 64, 64]              64\n",
      "           PReLU-177           [-1, 32, 64, 64]               1\n",
      "          Conv2d-178           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-179           [-1, 32, 64, 64]              64\n",
      "           PReLU-180           [-1, 32, 64, 64]               1\n",
      "          Conv2d-181          [-1, 128, 64, 64]           4,096\n",
      "     BatchNorm2d-182          [-1, 128, 64, 64]             256\n",
      "       Dropout2d-183          [-1, 128, 64, 64]               0\n",
      "           PReLU-184          [-1, 128, 64, 64]               1\n",
      "         RDDNeck-185          [-1, 128, 64, 64]               0\n",
      "          Conv2d-186           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-187           [-1, 32, 64, 64]              64\n",
      "           PReLU-188           [-1, 32, 64, 64]               1\n",
      "          Conv2d-189           [-1, 32, 64, 64]           5,120\n",
      "          Conv2d-190           [-1, 32, 64, 64]           5,120\n",
      "     BatchNorm2d-191           [-1, 32, 64, 64]              64\n",
      "           PReLU-192           [-1, 32, 64, 64]               1\n",
      "          Conv2d-193          [-1, 128, 64, 64]           4,096\n",
      "       Dropout2d-194          [-1, 128, 64, 64]               0\n",
      "     BatchNorm2d-195          [-1, 128, 64, 64]             256\n",
      "           PReLU-196          [-1, 128, 64, 64]               1\n",
      "          ASNeck-197          [-1, 128, 64, 64]               0\n",
      "          Conv2d-198           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-199           [-1, 32, 64, 64]              64\n",
      "           PReLU-200           [-1, 32, 64, 64]               1\n",
      "          Conv2d-201           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-202           [-1, 32, 64, 64]              64\n",
      "           PReLU-203           [-1, 32, 64, 64]               1\n",
      "          Conv2d-204          [-1, 128, 64, 64]           4,096\n",
      "     BatchNorm2d-205          [-1, 128, 64, 64]             256\n",
      "       Dropout2d-206          [-1, 128, 64, 64]               0\n",
      "           PReLU-207          [-1, 128, 64, 64]               1\n",
      "         RDDNeck-208          [-1, 128, 64, 64]               0\n",
      "          Conv2d-209           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-210           [-1, 32, 64, 64]              64\n",
      "           PReLU-211           [-1, 32, 64, 64]               1\n",
      "          Conv2d-212           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-213           [-1, 32, 64, 64]              64\n",
      "           PReLU-214           [-1, 32, 64, 64]               1\n",
      "          Conv2d-215          [-1, 128, 64, 64]           4,096\n",
      "     BatchNorm2d-216          [-1, 128, 64, 64]             256\n",
      "       Dropout2d-217          [-1, 128, 64, 64]               0\n",
      "           PReLU-218          [-1, 128, 64, 64]               1\n",
      "         RDDNeck-219          [-1, 128, 64, 64]               0\n",
      "          Conv2d-220           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-221           [-1, 32, 64, 64]              64\n",
      "           PReLU-222           [-1, 32, 64, 64]               1\n",
      "          Conv2d-223           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-224           [-1, 32, 64, 64]              64\n",
      "           PReLU-225           [-1, 32, 64, 64]               1\n",
      "          Conv2d-226          [-1, 128, 64, 64]           4,096\n",
      "     BatchNorm2d-227          [-1, 128, 64, 64]             256\n",
      "       Dropout2d-228          [-1, 128, 64, 64]               0\n",
      "           PReLU-229          [-1, 128, 64, 64]               1\n",
      "         RDDNeck-230          [-1, 128, 64, 64]               0\n",
      "          Conv2d-231           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-232           [-1, 32, 64, 64]              64\n",
      "           PReLU-233           [-1, 32, 64, 64]               1\n",
      "          Conv2d-234           [-1, 32, 64, 64]           5,120\n",
      "          Conv2d-235           [-1, 32, 64, 64]           5,120\n",
      "     BatchNorm2d-236           [-1, 32, 64, 64]              64\n",
      "           PReLU-237           [-1, 32, 64, 64]               1\n",
      "          Conv2d-238          [-1, 128, 64, 64]           4,096\n",
      "       Dropout2d-239          [-1, 128, 64, 64]               0\n",
      "     BatchNorm2d-240          [-1, 128, 64, 64]             256\n",
      "           PReLU-241          [-1, 128, 64, 64]               1\n",
      "          ASNeck-242          [-1, 128, 64, 64]               0\n",
      "          Conv2d-243           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-244           [-1, 32, 64, 64]              64\n",
      "           PReLU-245           [-1, 32, 64, 64]               1\n",
      "          Conv2d-246           [-1, 32, 64, 64]           9,248\n",
      "     BatchNorm2d-247           [-1, 32, 64, 64]              64\n",
      "           PReLU-248           [-1, 32, 64, 64]               1\n",
      "          Conv2d-249          [-1, 128, 64, 64]           4,096\n",
      "     BatchNorm2d-250          [-1, 128, 64, 64]             256\n",
      "       Dropout2d-251          [-1, 128, 64, 64]               0\n",
      "           PReLU-252          [-1, 128, 64, 64]               1\n",
      "         RDDNeck-253          [-1, 128, 64, 64]               0\n",
      " ConvTranspose2d-254           [-1, 32, 64, 64]           4,096\n",
      "     BatchNorm2d-255           [-1, 32, 64, 64]              64\n",
      "            ReLU-256           [-1, 32, 64, 64]               0\n",
      " ConvTranspose2d-257         [-1, 32, 128, 128]           9,216\n",
      "     BatchNorm2d-258         [-1, 32, 128, 128]              64\n",
      "            ReLU-259         [-1, 32, 128, 128]               0\n",
      " ConvTranspose2d-260         [-1, 64, 128, 128]           2,048\n",
      "     BatchNorm2d-261         [-1, 64, 128, 128]             128\n",
      "       Dropout2d-262         [-1, 64, 128, 128]               0\n",
      "          Conv2d-263           [-1, 64, 64, 64]           8,256\n",
      "     MaxUnpool2d-264         [-1, 64, 128, 128]               0\n",
      "            ReLU-265         [-1, 64, 128, 128]               0\n",
      "          UBNeck-266         [-1, 64, 128, 128]               0\n",
      "          Conv2d-267         [-1, 16, 128, 128]           1,024\n",
      "     BatchNorm2d-268         [-1, 16, 128, 128]              32\n",
      "            ReLU-269         [-1, 16, 128, 128]               0\n",
      "          Conv2d-270         [-1, 16, 128, 128]           2,320\n",
      "     BatchNorm2d-271         [-1, 16, 128, 128]              32\n",
      "            ReLU-272         [-1, 16, 128, 128]               0\n",
      "          Conv2d-273         [-1, 64, 128, 128]           1,024\n",
      "     BatchNorm2d-274         [-1, 64, 128, 128]             128\n",
      "       Dropout2d-275         [-1, 64, 128, 128]               0\n",
      "            ReLU-276         [-1, 64, 128, 128]               0\n",
      "         RDDNeck-277         [-1, 64, 128, 128]               0\n",
      "          Conv2d-278         [-1, 16, 128, 128]           1,024\n",
      "     BatchNorm2d-279         [-1, 16, 128, 128]              32\n",
      "            ReLU-280         [-1, 16, 128, 128]               0\n",
      "          Conv2d-281         [-1, 16, 128, 128]           2,320\n",
      "     BatchNorm2d-282         [-1, 16, 128, 128]              32\n",
      "            ReLU-283         [-1, 16, 128, 128]               0\n",
      "          Conv2d-284         [-1, 64, 128, 128]           1,024\n",
      "     BatchNorm2d-285         [-1, 64, 128, 128]             128\n",
      "       Dropout2d-286         [-1, 64, 128, 128]               0\n",
      "            ReLU-287         [-1, 64, 128, 128]               0\n",
      "         RDDNeck-288         [-1, 64, 128, 128]               0\n",
      " ConvTranspose2d-289         [-1, 16, 128, 128]           1,024\n",
      "     BatchNorm2d-290         [-1, 16, 128, 128]              32\n",
      "            ReLU-291         [-1, 16, 128, 128]               0\n",
      " ConvTranspose2d-292         [-1, 16, 256, 256]           2,304\n",
      "     BatchNorm2d-293         [-1, 16, 256, 256]              32\n",
      "            ReLU-294         [-1, 16, 256, 256]               0\n",
      " ConvTranspose2d-295         [-1, 16, 256, 256]             256\n",
      "     BatchNorm2d-296         [-1, 16, 256, 256]              32\n",
      "       Dropout2d-297         [-1, 16, 256, 256]               0\n",
      "          Conv2d-298         [-1, 16, 128, 128]           1,040\n",
      "     MaxUnpool2d-299         [-1, 16, 256, 256]               0\n",
      "            ReLU-300         [-1, 16, 256, 256]               0\n",
      "          UBNeck-301         [-1, 16, 256, 256]               0\n",
      "          Conv2d-302          [-1, 4, 256, 256]              64\n",
      "     BatchNorm2d-303          [-1, 4, 256, 256]               8\n",
      "            ReLU-304          [-1, 4, 256, 256]               0\n",
      "          Conv2d-305          [-1, 4, 256, 256]             148\n",
      "     BatchNorm2d-306          [-1, 4, 256, 256]               8\n",
      "            ReLU-307          [-1, 4, 256, 256]               0\n",
      "          Conv2d-308         [-1, 16, 256, 256]              64\n",
      "     BatchNorm2d-309         [-1, 16, 256, 256]              32\n",
      "       Dropout2d-310         [-1, 16, 256, 256]               0\n",
      "            ReLU-311         [-1, 16, 256, 256]               0\n",
      "         RDDNeck-312         [-1, 16, 256, 256]               0\n",
      " ConvTranspose2d-313         [-1, 12, 512, 512]           1,728\n",
      "================================================================\n",
      "Total params: 353,952\n",
      "Trainable params: 353,952\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 4193257.50\n",
      "Params size (MB): 1.35\n",
      "Estimated Total Size (MB): 4193261.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "\n",
    "summary(enet, input_size=(3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ENet - Real Time Semantic Segmentation.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
