{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qnyf_pzhKXv"
   },
   "source": [
    "## Install the dependencies and Import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5NCTHdEqj317"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3494787661\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "pl.seed_everything(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from torchmetrics import JaccardIndex\n",
    "import albumentations as A\n",
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "\n",
    "#from torch.optim.lr_scheduler import StepLR\n",
    "#import torch.optim.lr_scheduler as lr_scheduler\n",
    "from PIL import Image\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('medium' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WANDB SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb sweep setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'acc',\n",
    "    'goal': 'maximize'\n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']\n",
    "        },\n",
    "    'batch_size':{\n",
    "        'values':[5,10,15,20]\n",
    "    },\n",
    "    'learning_rate': {\n",
    "    'distribution': 'log_uniform_values',\n",
    "    'min': 5e-6,\n",
    "    'max': 5e-2\n",
    "    },\n",
    "    'image_ip_size':{\n",
    "        'values': [224,512]\n",
    "    }\n",
    "    }\n",
    "\n",
    "parameters_dict.update({\n",
    "    'epochs':{\n",
    "        'value': 5\n",
    "    }\n",
    "})\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'acc'},\n",
      " 'parameters': {'batch_size': {'values': [5, 10, 15, 20]},\n",
      "                'epochs': {'value': 5},\n",
      "                'image_ip_size': {'values': [224, 512]},\n",
      "                'learning_rate': {'distribution': 'log_uniform_values',\n",
      "                                  'max': 0.05,\n",
      "                                  'min': 5e-06},\n",
      "                'optimizer': {'values': ['adam', 'sgd']}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: a6tls5ai\n",
      "Sweep URL: https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project ='try3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET SECTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_train(img_size):\n",
    "    return A.Compose([\n",
    "                        A.Resize(img_size,img_size),\n",
    "                        A.HorizontalFlip(p= 0.5)\n",
    "                    ])\n",
    "    \n",
    "def aug_val(img_size): \n",
    "    return A.Resize(img_size, img_size)\n",
    "   \n",
    "def aug_test(img_size):\n",
    "    return A.Resize(img_size, img_size)\n",
    "\n",
    "def norm_transform():\n",
    "    return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.3576, 0.3713, 0.3657], std=[0.2608, 0.2723, 0.2943])\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dataset_normalization(loader):\n",
    "    # Initialize accumulators\n",
    "    channel_sum = torch.tensor([0.0, 0.0, 0.0])\n",
    "    channel_squared_sum = torch.tensor([0.0, 0.0, 0.0])\n",
    "    num_batches = 0\n",
    "\n",
    "    for images, _ in loader:\n",
    "        # Accumulate sum and squared sum for each channel\n",
    "        channel_sum += images.sum(dim=[0, 2, 3])\n",
    "        channel_squared_sum += (images ** 2).sum(dim=[0, 2, 3])\n",
    "        num_batches += images.shape[0]\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    mean = channel_sum / (num_batches *224  *224 )\n",
    "    std = (channel_squared_sum / (num_batches * 224 *224 ) - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, training_path, segmented_path,road_idx,norm_transform, augment):\n",
    "        #\"C:\\Users\\dalex\\Desktop\\Daya\\datasets\\idd-lite\\idd20k_lite\\leftImg8bit\\train\\2\\593144_image.jpg\"\n",
    "        ##print('paths',training_path,segmented_path)\n",
    "        \n",
    "        self.filenames_t = glob.glob(training_path+f'*\\*.jpg')\n",
    "        ##print(self.filenames_t)\n",
    "        self.filenames_s = glob.glob(segmented_path+f'*\\*[0-9]_label.png')\n",
    "        ##print(self.filenames_s)\n",
    "        self.norm_transform = norm_transform\n",
    "        self.augment = augment\n",
    "        self.road_idx = road_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames_t)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print('inside dataset')\n",
    "        img_path = self.filenames_t[idx]\n",
    "        mask_path = self.filenames_s[idx]\n",
    "        img = Image.open(img_path).convert('RGB')# as PIL is used image is read as channel, height, width\n",
    "        label_array = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)# image is read as height, width\n",
    "\n",
    "        #extracting mask of road using road_idx\n",
    "        road_mask = np.zeros_like(label_array)\n",
    "        road_mask[label_array == self.road_idx] = 1\n",
    "        road_mask = np.expand_dims(road_mask,axis = -1)\n",
    "        \n",
    "        if self.augment:\n",
    "            img, mask = self.augment(img, road_mask)\n",
    "\n",
    "        if self.norm_transform:\n",
    "            img = self.norm_transform(img)\n",
    "            #print('normalisation done')\n",
    "            \n",
    "        if not isinstance(mask, torch.Tensor):\n",
    "            mask = torch.tensor(mask, dtype=torch.uint8)\n",
    "            #print('mask turned to tensor')\n",
    "            \n",
    "        mask = mask.permute(2, 0, 1)\n",
    "        #print('batched input and labels')\n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class idd_lite_datamodule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,image_ip_size,batch_size):\n",
    "        super().__init__()\n",
    "        self.image_ip_size = image_ip_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "    def setup(self,stage=None):\n",
    "        self.train_dataset = CustomDataset(\"c:\\\\Users\\\\dalex\\\\Desktop\\\\Daya\\\\datasets\\\\idd-lite\\\\idd20k_lite\\\\leftImg8bit\\\\train\\\\\",\n",
    "                                    \"c:\\\\Users\\\\dalex\\\\Desktop\\\\Daya\\\\datasets\\\\idd-lite\\\\idd20k_lite\\\\gtFine\\\\train\\\\\", \n",
    "                                    road_idx=0,\n",
    "                                    norm_transform= norm_transform(),\n",
    "                                    augment=aug_train(self.image_ip_size))\n",
    "\n",
    "\n",
    "        self.val_dataset = CustomDataset(\"c:\\\\Users\\\\dalex\\\\Desktop\\\\Daya\\\\datasets\\\\idd-lite\\\\idd20k_lite\\\\leftImg8bit\\\\val\\\\\",\n",
    "                                    \"c:\\\\Users\\\\dalex\\\\Desktop\\\\Daya\\\\datasets\\\\idd-lite\\\\idd20k_lite\\\\gtFine\\\\val\\\\\", \n",
    "                                    road_idx=0,\n",
    "                                    norm_transform= norm_transform(),\n",
    "                                    augment=aug_val(self.image_ip_size))\n",
    "        \n",
    "        \n",
    "        #print('preprocesiing setup')\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        #print('inside train dataloader')\n",
    "        # Instantiate your dataset\n",
    "        \n",
    "        tr_data_loader  = DataLoader(self.train_dataset, batch_size= self.batch_size, shuffle=True)\n",
    "        \n",
    "        return tr_data_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "    \n",
    "        # Instantiate your dataset\n",
    "        \n",
    "        val_data_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False) \n",
    "\n",
    "        return val_data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there is any gpu available and pass the model to gpu or cpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i26TZVXmhewY"
   },
   "source": [
    "## MODEL DEFINITIONS\n",
    "\n",
    "trying out variations between joined training and seperated training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqHUezLfPBwn"
   },
   "outputs": [],
   "source": [
    "class InitialBlock(nn.Module):\n",
    "  \n",
    "  # Initial block of the model:\n",
    "  #         Input\n",
    "  #        /     \\\n",
    "  #       /       \\\n",
    "  #maxpool2d    conv2d-3x3\n",
    "  #       \\       /  \n",
    "  #        \\     /\n",
    "  #      concatenate\n",
    "   \n",
    "    def __init__ (self,in_channels = 3,out_channels = 13):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, \n",
    "                                      stride = 2, \n",
    "                                      padding = 0)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, \n",
    "                                out_channels,\n",
    "                                kernel_size = 3,\n",
    "                                stride = 2, \n",
    "                                padding = 1)\n",
    "\n",
    "        self.prelu = nn.PReLU(16)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        \n",
    "        main = self.conv(x)\n",
    "        main = self.batchnorm(main)\n",
    "        \n",
    "        side = self.maxpool(x)\n",
    "        \n",
    "        # concatenating on the channels axis\n",
    "        x = torch.cat((main, side), dim=1)\n",
    "        x = self.prelu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERqXRpl_sfSE"
   },
   "outputs": [],
   "source": [
    "class UBNeck(nn.Module):\n",
    "    \n",
    "  # Upsampling bottleneck:\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  # conv2d-1x1     convTrans2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         convTrans2d-3x3\n",
    "  #      |             | PReLU\n",
    "  #      |         convTrans2d-1x1\n",
    "  #      |             |\n",
    "  # maxunpool2d    Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  #  Params: \n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
    "  \n",
    "    def __init__(self, in_channels, out_channels, relu=False, projection_ratio=4):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        self.reduced_depth = int(in_channels / projection_ratio)\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        \n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size = 2,\n",
    "                                     stride = 2)\n",
    "        \n",
    "        self.main_conv = nn.Conv2d(in_channels = self.in_channels,\n",
    "                                    out_channels = self.out_channels,\n",
    "                                    kernel_size = 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        \n",
    "        \n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False)\n",
    "        \n",
    "        \n",
    "        self.prelu1 = activation\n",
    "        \n",
    "        # This layer used for Upsampling\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = 2,\n",
    "                                  padding = 1,\n",
    "                                  output_padding = 1,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu2 = activation\n",
    "        \n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu3 = activation\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "    def forward(self, x, indices):\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.convt1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.convt2(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.convt3(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        \n",
    "        x_copy = self.main_conv(x_copy)\n",
    "        x_copy = self.unpool(x_copy, indices, output_size=x.size())\n",
    "        \n",
    "        # summing the main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-nTIAS9bd9z"
   },
   "outputs": [],
   "source": [
    "class RDDNeck(nn.Module):\n",
    "    def __init__(self, dilation, in_channels, out_channels, down_flag, relu=False, projection_ratio=4, p=0.1):\n",
    "      \n",
    "  # Regular|Dilated|Downsampling bottlenecks:\n",
    "  #\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  # maxpooling2d   conv2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-3x3\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             |\n",
    "  #  Padding2d     Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  # Params: \n",
    "  #  dilation (bool) - if True: creating dilation bottleneck\n",
    "  #  down_flag (bool) - if True: creating downsampling bottleneck\n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
    "  #  p - dropout ratio\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = dilation\n",
    "        self.down_flag = down_flag\n",
    "        \n",
    "        # calculating the number of reduced channels\n",
    "        if down_flag:\n",
    "            self.stride = 2\n",
    "            self.reduced_depth = int(in_channels // projection_ratio)\n",
    "        else:\n",
    "            self.stride = 1\n",
    "            self.reduced_depth = int(out_channels // projection_ratio)\n",
    "        \n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2,\n",
    "                                      stride = 2,\n",
    "                                      padding = 0, return_indices=True)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=p)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False,\n",
    "                               dilation = 1)\n",
    "        \n",
    "        self.prelu1 = activation\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = self.stride,\n",
    "                                  padding = self.dilation,\n",
    "                                  bias = True,\n",
    "                                  dilation = self.dilation)\n",
    "                                  \n",
    "        self.prelu2 = activation\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  stride = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False,\n",
    "                                  dilation = 1)\n",
    "        \n",
    "        self.prelu3 = activation\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        bs = x.size()[0]\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm2(x)\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        if self.down_flag:\n",
    "            x_copy, indices = self.maxpool(x_copy)\n",
    "          \n",
    "        if self.in_channels != self.out_channels:\n",
    "            out_shape = self.out_channels - self.in_channels\n",
    "            \n",
    "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
    "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
    "            extras = extras.to(device)\n",
    "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
    "\n",
    "        # Summing main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        if self.down_flag:\n",
    "            return x, indices\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tb_i1sCvtmMF"
   },
   "outputs": [],
   "source": [
    "class ASNeck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, projection_ratio=4):\n",
    "      \n",
    "  # Asymetric bottleneck:\n",
    "  #\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x5\n",
    "  #      |             |\n",
    "  #      |         conv2d-5x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             |\n",
    "  #  Padding2d     Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  # Params:    \n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        self.reduced_depth = int(in_channels / projection_ratio)\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False)\n",
    "        \n",
    "        self.prelu1 = nn.PReLU()\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = (1, 5),\n",
    "                                  stride = 1,\n",
    "                                  padding = (0, 2),\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.conv22 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = (5, 1),\n",
    "                                  stride = 1,\n",
    "                                  padding = (2, 0),\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu2 = nn.PReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  stride = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu3 = nn.PReLU()\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.size()[0]\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.conv21(x)\n",
    "        x = self.conv22(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        \n",
    "        if self.in_channels != self.out_channels:\n",
    "            out_shape = self.out_channels - self.in_channels\n",
    "            \n",
    "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
    "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
    "            extras = extras.to(x_copy.device)\n",
    "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
    "        \n",
    "        # Summing main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1pz_bve690y"
   },
   "outputs": [],
   "source": [
    "class joined_ENet(pl.LightningModule):\n",
    "  \n",
    "  # Creating Enet model!\n",
    "  \n",
    "    def __init__(self, lr,optim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = 1\n",
    "        self.lr = lr\n",
    "        self.optimizer_name = optim\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_acc = JaccardIndex(task = 'multiclass', num_classes = 2)\n",
    "        self.val_acc = JaccardIndex(task = 'multiclass', num_classes = 2)\n",
    "        self.test_acc = JaccardIndex(task = 'multiclass', num_classes = 2)\n",
    "\n",
    "        # The initial block\n",
    "        self.init = InitialBlock()\n",
    "        \n",
    "        \n",
    "        # The first bottleneck\n",
    "        self.b10 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=64, \n",
    "                           down_flag=True, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b11 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b12 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b13 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b14 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        \n",
    "        # The second bottleneck\n",
    "        self.b20 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=128, \n",
    "                           down_flag=True)\n",
    "        \n",
    "        self.b21 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b22 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b23 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b24 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b25 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b26 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b27 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b28 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The third bottleneck\n",
    "        self.b31 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b32 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b33 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b34 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b35 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b36 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b37 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b38 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        self.b40 = UBNeck(in_channels=128, \n",
    "                          out_channels=64, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b41 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        self.b42 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        self.b50 = UBNeck(in_channels=64, \n",
    "                          out_channels=16, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b51 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=16, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n",
    "                                           out_channels=self.C, \n",
    "                                           kernel_size=3, \n",
    "                                           stride=2, \n",
    "                                           padding=1, \n",
    "                                           output_padding=1,\n",
    "                                           bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # The initial block\n",
    "        x = self.init(x)\n",
    "        \n",
    "        # The first bottleneck\n",
    "        x, i1 = self.b10(x)\n",
    "        x = self.b11(x)\n",
    "        x = self.b12(x)\n",
    "        x = self.b13(x)\n",
    "        x = self.b14(x)\n",
    "        \n",
    "        # The second bottleneck\n",
    "        x, i2 = self.b20(x)\n",
    "        x = self.b21(x)\n",
    "        x = self.b22(x)\n",
    "        x = self.b23(x)\n",
    "        x = self.b24(x)\n",
    "        x = self.b25(x)\n",
    "        x = self.b26(x)\n",
    "        x = self.b27(x)\n",
    "        x = self.b28(x)\n",
    "        \n",
    "        # The third bottleneck\n",
    "        x = self.b31(x)\n",
    "        x = self.b32(x)\n",
    "        x = self.b33(x)\n",
    "        x = self.b34(x)\n",
    "        x = self.b35(x)\n",
    "        x = self.b36(x)\n",
    "        x = self.b37(x)\n",
    "        x = self.b38(x)\n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        x = self.b40(x, i2)\n",
    "        x = self.b41(x)\n",
    "        x = self.b42(x)\n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        x = self.b50(x, i1)\n",
    "        x = self.b51(x)\n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        x = self.fullconv(x)\n",
    "        #print('finished parsing through model')\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def loss(self, inputs, target):\n",
    "        if target.dtype == torch.uint8:\n",
    "            target = target.float()\n",
    "\n",
    "        #print(inputs.shape)\n",
    "        #print(target.shape)\n",
    "        pred = self(inputs.float()) \n",
    "        \n",
    "        loss1 = DiceLoss(mode='binary')(pred, target.float())\n",
    "        loss2 = nn.BCEWithLogitsLoss()(pred, target.float())\n",
    "\n",
    "        #print((loss1+loss2).shape)\n",
    "        return (loss1 + loss2), pred   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "        \n",
    "        # The initial block\n",
    "        self.init_layer = InitialBlock()\n",
    "        \n",
    "        \n",
    "        # The first bottleneck\n",
    "        self.b10 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=64, \n",
    "                           down_flag=True, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b11 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b12 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b13 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b14 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        \n",
    "        # The second bottleneck\n",
    "        self.b20 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=128, \n",
    "                           down_flag=True)\n",
    "        \n",
    "        self.b21 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b22 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b23 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b24 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b25 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b26 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b27 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b28 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The third bottleneck\n",
    "        self.b31 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b32 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b33 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b34 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b35 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b36 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b37 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b38 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # The initial block\n",
    "        x = self.init_layer(x)\n",
    "        \n",
    "        # The first bottleneck\n",
    "        x, i1 = self.b10(x)\n",
    "        x = self.b11(x)\n",
    "        x = self.b12(x)\n",
    "        x = self.b13(x)\n",
    "        x = self.b14(x)\n",
    "        \n",
    "        # The second bottleneck\n",
    "        x, i2 = self.b20(x)\n",
    "        x = self.b21(x)\n",
    "        x = self.b22(x)\n",
    "        x = self.b23(x)\n",
    "        x = self.b24(x)\n",
    "        x = self.b25(x)\n",
    "        x = self.b26(x)\n",
    "        x = self.b27(x)\n",
    "        x = self.b28(x)\n",
    "        \n",
    "        # The third bottleneck\n",
    "        x = self.b31(x)\n",
    "        x = self.b32(x)\n",
    "        x = self.b33(x)\n",
    "        x = self.b34(x)\n",
    "        x = self.b35(x)\n",
    "        x = self.b36(x)\n",
    "        x = self.b37(x)\n",
    "        x = self.b38(x)\n",
    "\n",
    "        return x, i1,i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(nn.Module):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "\n",
    "\n",
    "        # The fourth bottleneck\n",
    "        self.b40 = UBNeck(in_channels=128, \n",
    "                          out_channels=64, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b41 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        self.b42 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        self.b50 = UBNeck(in_channels=64, \n",
    "                          out_channels=16, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b51 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=16, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n",
    "                                           out_channels=self.C, \n",
    "                                           kernel_size=3, \n",
    "                                           stride=2, \n",
    "                                           padding=1, \n",
    "                                           output_padding=1,\n",
    "                                           bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x, i1, i2):\n",
    "        # The fourth bottleneck\n",
    "        x = self.b40(x, i2)\n",
    "        x = self.b41(x)\n",
    "        x = self.b42(x)\n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        x = self.b50(x, i1)\n",
    "        x = self.b51(x)\n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        x = self.fullconv(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER-DECODER\n",
    "class enc_dec_ENet(pl.LightningModule):\n",
    "  \n",
    "  # Creating Enet model!\n",
    "  \n",
    "    def __init__(self,encoder,decoder, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder(C)\n",
    "        \n",
    "        # Create the decoder\n",
    "        self.decoder = decoder(C)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_acc = JaccardIndex(task = 'multiclass', num_classes = 2)\n",
    "        self.val_acc = JaccardIndex(task = 'multiclass', num_classes = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the encoder\n",
    "        encoder_output, i1, i2 = self.encoder(x)\n",
    "\n",
    "        # Pass encoder output and intermediate feature maps to the decoder\n",
    "        decoder_output = self.decoder(encoder_output, i1, i2)\n",
    "        #print('finished parsing through the model')\n",
    "        return decoder_output\n",
    "\n",
    "    def combined_loss(self, inputs, target):\n",
    "        #print('inside loss function')\n",
    "        if target.dtype == torch.uint8:\n",
    "            target = target.float()\n",
    "\n",
    "        pred = self(inputs.float()) \n",
    "        #print('predictions found')\n",
    "        loss1 = DiceLoss(mode='binary')(pred, target)\n",
    "        loss2 = nn.BCEWithLogitsLoss()(pred, target)\n",
    "        return (loss1 + loss2), pred\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(self):\n",
    "    #print('inside optimiser function')\n",
    "    if self.optimizer_name =='adam':\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=2e-4)\n",
    "    elif self.optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(self.parameters(),\n",
    "                                lr=self.lr, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {self.optimizer_name}\")\n",
    "    #print('optimizer ready')\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFIdlVWviBYl"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "WQ6XJzl6Ta1_",
    "outputId": "a3f62522-391d-4e05-f138-11c78a0d90cf"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Train loop\n",
    "def training_step(self,batch, batch_idx):\n",
    "    #print('inside train function')\n",
    "    X_batch, mask_batch = batch\n",
    "    #print('batch selected')\n",
    "    net_loss, preds = self.loss(X_batch, mask_batch.float())\n",
    "    #print('loss computed')\n",
    "    preds_ohe = torch.argmax(preds, dim=1)\n",
    "    mask_batch_1d = mask_batch.squeeze(1)\n",
    "    \n",
    "    self.train_acc(preds_ohe,mask_batch_1d)\n",
    "    #print('accuracy computed')\n",
    "    self.log('train/loss', net_loss, on_epoch = True )\n",
    "    self.log('train/acc', self.train_acc, on_epoch = True, logger=True)\n",
    "\n",
    "    return net_loss#loss must be returned to facilitate grad calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(self, batch, batch_idx):\n",
    "    X_batch, mask_batch = batch\n",
    "    net_loss, preds = self.loss(X_batch, mask_batch.float())\n",
    "    preds_ohe =torch.argmax(preds, dim = 1)\n",
    "    mask_batch_1d = mask_batch.squeeze(1)\n",
    "    self.test_acc(preds_ohe, mask_batch_1d)\n",
    "\n",
    "    self.log('test/loss', net_loss,on_step = False, on_epoch =True)\n",
    "    self.log('test/acc', self.test_acc, on_step = False, on_epoch = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_test_epoch_end(self):\n",
    "    dummy_input = torch.zeros_like(self.img_aug_func_train, device = self.device)\n",
    "    model_filename = 'enet.onnx'\n",
    "    self.to_onnx(model_filename, dummy_input, export_params= True)\n",
    "    artifact = wandb.Artifact('model.ckpt', type = 'model')\n",
    "    artifact.add_file(model_filename)\n",
    "    wandb.log_artifact(artifact)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging histogram of logits during validation\n",
    "def on_validation_epoch_start(self):\n",
    "    self.val_steps_output = []#to store all the logits\n",
    "\n",
    "def validation_step(self, batch, batch_idx):\n",
    "    X_batch, mask_batch = batch\n",
    "    net_loss, preds = self.loss(X_batch, mask_batch.float())\n",
    "    preds_ohe =torch.argmax(preds, dim = 1)\n",
    "    mask_batch_1d = mask_batch.squeeze(1)\n",
    "    self.val_acc(preds_ohe, mask_batch_1d)\n",
    "\n",
    "    self.log('val/loss_epoch', net_loss,on_step = False, on_epoch =True)\n",
    "    self.log('val/acc_loss_epoch', self.test_acc, on_step = False, on_epoch = True)\n",
    "    self.val_steps_output.append(preds)\n",
    "    return preds\n",
    "\n",
    "def on_validation_epoch_end(self):\n",
    "    validation_step_outputs = self.val_steps_output\n",
    "\n",
    "    dummy_input = torch.zeros_like(self.img_aug_func_train, device = self.device)\n",
    "    model_filename = f\"enet_model_{str(self.global_step).zfill(5)}.onnx\"\n",
    "    torch.onnx.export(self, dummy_input, model_filename, opset_version=11)\n",
    "    artifact = wandb.Artifact(name=\"model.ckpt\", type=\"model\")\n",
    "    artifact.add_file(model_filename)\n",
    "    self.logger.experiment.log_artifact(artifact)\n",
    "\n",
    "    flattened_logits = torch.flatten(torch.cat(validation_step_outputs))\n",
    "    self.logger.experiment.log(\n",
    "        {\"valid/logits\": wandb.Histogram(flattened_logits.to(\"cpu\")),\n",
    "         \"global_step\": self.global_step})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback with image viewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePredictionLogger(pl.Callback):\n",
    "    def __init__(self, val_samples, num_samples=10):\n",
    "        super().__init__()\n",
    "        self.inputs, self.gt = val_samples\n",
    "        self.inputs = self.inputs[:num_samples]\n",
    "        self.gt = self.gt[:num_samples]\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        self.inputs = self.inputs.to(pl_module.device)\n",
    "        pred = pl_module(self.inputs)\n",
    "        pred_segmap = torch.argmax(pred,1)\n",
    "        \n",
    "        data = []\n",
    "\n",
    "        for x, y, z in zip(self.inputs, pred_segmap, self.gt):\n",
    "            x=x.squeeze(0).cpu().detach().numpy()\n",
    "            x = x.transpose(1,2,0)\n",
    "            y=y.squeeze(0).cpu().detach().numpy()\n",
    "            colormap = plt.get_cmap('viridis')  # Or any other colormap\n",
    "            y_colored = colormap(y)[:, :, :3]   \n",
    "            \n",
    "            z=z.squeeze(0).cpu().detach().numpy()\n",
    "            z_colored = colormap(y)[:,:,:3]\n",
    "\n",
    "            logged_x = wandb.Image(x) \n",
    "            logged_y = wandb.Image(y_colored)\n",
    "            logged_z = wandb.Image(z_colored)\n",
    "\n",
    "            data.append([logged_x,logged_y,logged_z])\n",
    "\n",
    "        trainer.logger.experiment.log({\n",
    "            'val images table':wandb.Table(data = data, columns =['Inputs', 'Predictions', 'Ground Truth'])\n",
    "\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_using_wandb(config = None):\n",
    "    with wandb.init(config = config):\n",
    "        config = wandb.config# automatically set by sweepcontroller previously configuired\n",
    "        logger = WandbLogger(project='try3')\n",
    "        \n",
    "        idd_data = idd_lite_datamodule(config.image_ip_size,config.batch_size)\n",
    "        \n",
    "        idd_data.setup()\n",
    "        joined_ENet.training_step = training_step\n",
    "        joined_ENet.configure_optimizers = configure_optimizers\n",
    "        joined_ENet.on_validation_epoch_start = on_validation_epoch_start\n",
    "        joined_ENet.validation_step = validation_step\n",
    "        joined_ENet.on_validation_epoch_end = on_validation_epoch_end\n",
    "\n",
    "        \n",
    "        model = joined_ENet(config.learning_rate, config.optimizer)        \n",
    "        wandb.watch(model,model.loss, log='all', log_freq= 10)\n",
    "        samples = next(iter(idd_data.val_dataloader()))\n",
    "\n",
    "\n",
    "        trainer = pl.Trainer(\n",
    "        logger = logger,\n",
    "        log_every_n_steps = 50,\n",
    "        max_epochs = config.epochs,\n",
    "        deterministic = False,\n",
    "        callbacks = [ImagePredictionLogger(samples)]\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, datamodule=idd_data)\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x5ed34m2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_ip_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.039570078615655264\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\wandb\\run-20240228_045301-x5ed34m2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dayaalex/try3/runs/x5ed34m2' target=\"_blank\">spring-sweep-4</a></strong> to <a href='https://wandb.ai/dayaalex/try3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai' target=\"_blank\">https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dayaalex/try3' target=\"_blank\">https://wandb.ai/dayaalex/try3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai' target=\"_blank\">https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dayaalex/try3/runs/x5ed34m2' target=\"_blank\">https://wandb.ai/dayaalex/try3/runs/x5ed34m2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type                   | Params\n",
      "------------------------------------------------------\n",
      "0  | train_acc | MulticlassJaccardIndex | 0     \n",
      "1  | val_acc   | MulticlassJaccardIndex | 0     \n",
      "2  | test_acc  | MulticlassJaccardIndex | 0     \n",
      "3  | init      | InitialBlock           | 406   \n",
      "4  | b10       | RDDNeck                | 605   \n",
      "5  | b11       | RDDNeck                | 4.5 K \n",
      "6  | b12       | RDDNeck                | 4.5 K \n",
      "7  | b13       | RDDNeck                | 4.5 K \n",
      "8  | b14       | RDDNeck                | 4.5 K \n",
      "9  | b20       | RDDNeck                | 5.7 K \n",
      "10 | b21       | RDDNeck                | 17.8 K\n",
      "11 | b22       | RDDNeck                | 17.8 K\n",
      "12 | b23       | ASNeck                 | 18.8 K\n",
      "13 | b24       | RDDNeck                | 17.8 K\n",
      "14 | b25       | RDDNeck                | 17.8 K\n",
      "15 | b26       | RDDNeck                | 17.8 K\n",
      "16 | b27       | ASNeck                 | 18.8 K\n",
      "17 | b28       | RDDNeck                | 17.8 K\n",
      "18 | b31       | RDDNeck                | 17.8 K\n",
      "19 | b32       | RDDNeck                | 17.8 K\n",
      "20 | b33       | ASNeck                 | 18.8 K\n",
      "21 | b34       | RDDNeck                | 17.8 K\n",
      "22 | b35       | RDDNeck                | 17.8 K\n",
      "23 | b36       | RDDNeck                | 17.8 K\n",
      "24 | b37       | ASNeck                 | 18.8 K\n",
      "25 | b38       | RDDNeck                | 17.8 K\n",
      "26 | b40       | UBNeck                 | 23.8 K\n",
      "27 | b41       | RDDNeck                | 4.5 K \n",
      "28 | b42       | RDDNeck                | 4.5 K \n",
      "29 | b50       | UBNeck                 | 4.7 K \n",
      "30 | b51       | RDDNeck                | 316   \n",
      "31 | fullconv  | ConvTranspose2d        | 144   \n",
      "------------------------------------------------------\n",
      "350 K     Trainable params\n",
      "0         Non-trainable params\n",
      "350 K     Total params\n",
      "1.404     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  6.74it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\2073484368.py\", line 30, in train_using_wandb\n",
      "    trainer.fit(model, datamodule=idd_data)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1030, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1059, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py\", line 182, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 142, in run\n",
      "    return self.on_run_end()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 254, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 334, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\4263242126.py\", line 20, in on_validation_epoch_end\n",
      "    dummy_input = torch.zeros_like(self.img_aug_func_train, device = self.device)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "AttributeError: 'joined_ENet' object has no attribute 'img_aug_func_train'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">spring-sweep-4</strong> at: <a href='https://wandb.ai/dayaalex/try3/runs/x5ed34m2' target=\"_blank\">https://wandb.ai/dayaalex/try3/runs/x5ed34m2</a><br/>Synced 5 W&B file(s), 1 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240228_045301-x5ed34m2\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run x5ed34m2 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\2073484368.py\", line 30, in train_using_wandb\n",
      "    trainer.fit(model, datamodule=idd_data)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1030, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1059, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py\", line 182, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 142, in run\n",
      "    return self.on_run_end()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 254, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 334, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\4263242126.py\", line 20, in on_validation_epoch_end\n",
      "    dummy_input = torch.zeros_like(self.img_aug_func_train, device = self.device)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "AttributeError: 'joined_ENet' object has no attribute 'img_aug_func_train'\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run x5ed34m2 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\2073484368.py\", line 30, in train_using_wandb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, datamodule=idd_data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1030, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1059, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py\", line 182, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 142, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.on_run_end()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 254, in on_run_end\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._on_evaluation_epoch_end()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 334, in _on_evaluation_epoch_end\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_lightning_module_hook(trainer, hook_name)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\4263242126.py\", line 20, in on_validation_epoch_end\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     dummy_input = torch.zeros_like(self.img_aug_func_train, device = self.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AttributeError: 'joined_ENet' object has no attribute 'img_aug_func_train'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: iv3vcqyd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 15\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_ip_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.782121010681869e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2165: UserWarning: Run (c7zcf5gg) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stdout\", data),\n",
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2165: UserWarning: Run (ymhejg6l) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stdout\", data),\n",
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2165: UserWarning: Run (e1ebtp1z) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stdout\", data),\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\wandb\\run-20240228_045312-iv3vcqyd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dayaalex/try3/runs/iv3vcqyd' target=\"_blank\">unique-sweep-5</a></strong> to <a href='https://wandb.ai/dayaalex/try3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai' target=\"_blank\">https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dayaalex/try3' target=\"_blank\">https://wandb.ai/dayaalex/try3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai' target=\"_blank\">https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dayaalex/try3/runs/iv3vcqyd' target=\"_blank\">https://wandb.ai/dayaalex/try3/runs/iv3vcqyd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type                   | Params\n",
      "------------------------------------------------------\n",
      "0  | train_acc | MulticlassJaccardIndex | 0     \n",
      "1  | val_acc   | MulticlassJaccardIndex | 0     \n",
      "2  | test_acc  | MulticlassJaccardIndex | 0     \n",
      "3  | init      | InitialBlock           | 406   \n",
      "4  | b10       | RDDNeck                | 605   \n",
      "5  | b11       | RDDNeck                | 4.5 K \n",
      "6  | b12       | RDDNeck                | 4.5 K \n",
      "7  | b13       | RDDNeck                | 4.5 K \n",
      "8  | b14       | RDDNeck                | 4.5 K \n",
      "9  | b20       | RDDNeck                | 5.7 K \n",
      "10 | b21       | RDDNeck                | 17.8 K\n",
      "11 | b22       | RDDNeck                | 17.8 K\n",
      "12 | b23       | ASNeck                 | 18.8 K\n",
      "13 | b24       | RDDNeck                | 17.8 K\n",
      "14 | b25       | RDDNeck                | 17.8 K\n",
      "15 | b26       | RDDNeck                | 17.8 K\n",
      "16 | b27       | ASNeck                 | 18.8 K\n",
      "17 | b28       | RDDNeck                | 17.8 K\n",
      "18 | b31       | RDDNeck                | 17.8 K\n",
      "19 | b32       | RDDNeck                | 17.8 K\n",
      "20 | b33       | ASNeck                 | 18.8 K\n",
      "21 | b34       | RDDNeck                | 17.8 K\n",
      "22 | b35       | RDDNeck                | 17.8 K\n",
      "23 | b36       | RDDNeck                | 17.8 K\n",
      "24 | b37       | ASNeck                 | 18.8 K\n",
      "25 | b38       | RDDNeck                | 17.8 K\n",
      "26 | b40       | UBNeck                 | 23.8 K\n",
      "27 | b41       | RDDNeck                | 4.5 K \n",
      "28 | b42       | RDDNeck                | 4.5 K \n",
      "29 | b50       | UBNeck                 | 4.7 K \n",
      "30 | b51       | RDDNeck                | 316   \n",
      "31 | fullconv  | ConvTranspose2d        | 144   \n",
      "------------------------------------------------------\n",
      "350 K     Trainable params\n",
      "0         Non-trainable params\n",
      "350 K     Total params\n",
      "1.404     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\2073484368.py\", line 30, in train_using_wandb\n",
      "    trainer.fit(model, datamodule=idd_data)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1030, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1059, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py\", line 182, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 142, in run\n",
      "    return self.on_run_end()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 254, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 334, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\4263242126.py\", line 20, in on_validation_epoch_end\n",
      "    dummy_input = torch.zeros_like(self.img_aug_func_train, device = self.device)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "AttributeError: 'joined_ENet' object has no attribute 'img_aug_func_train'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-5</strong> at: <a href='https://wandb.ai/dayaalex/try3/runs/iv3vcqyd' target=\"_blank\">https://wandb.ai/dayaalex/try3/runs/iv3vcqyd</a><br/>Synced 5 W&B file(s), 1 media file(s), 12 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240228_045312-iv3vcqyd\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run iv3vcqyd errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\2073484368.py\", line 30, in train_using_wandb\n",
      "    trainer.fit(model, datamodule=idd_data)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1030, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1059, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py\", line 182, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 142, in run\n",
      "    return self.on_run_end()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 254, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 334, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\4263242126.py\", line 20, in on_validation_epoch_end\n",
      "    dummy_input = torch.zeros_like(self.img_aug_func_train, device = self.device)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "AttributeError: 'joined_ENet' object has no attribute 'img_aug_func_train'\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run iv3vcqyd errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\2073484368.py\", line 30, in train_using_wandb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, datamodule=idd_data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1030, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1059, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py\", line 182, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 142, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.on_run_end()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 254, in on_run_end\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._on_evaluation_epoch_end()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 334, in _on_evaluation_epoch_end\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_lightning_module_hook(trainer, hook_name)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\4263242126.py\", line 20, in on_validation_epoch_end\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     dummy_input = torch.zeros_like(self.img_aug_func_train, device = self.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AttributeError: 'joined_ENet' object has no attribute 'img_aug_func_train'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pnz4n8de with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_ip_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.000460806373826343\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\wandb\\run-20240228_045322-pnz4n8de</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dayaalex/try3/runs/pnz4n8de' target=\"_blank\">icy-sweep-6</a></strong> to <a href='https://wandb.ai/dayaalex/try3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai' target=\"_blank\">https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dayaalex/try3' target=\"_blank\">https://wandb.ai/dayaalex/try3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai' target=\"_blank\">https://wandb.ai/dayaalex/try3/sweeps/a6tls5ai</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dayaalex/try3/runs/pnz4n8de' target=\"_blank\">https://wandb.ai/dayaalex/try3/runs/pnz4n8de</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type                   | Params\n",
      "------------------------------------------------------\n",
      "0  | train_acc | MulticlassJaccardIndex | 0     \n",
      "1  | val_acc   | MulticlassJaccardIndex | 0     \n",
      "2  | test_acc  | MulticlassJaccardIndex | 0     \n",
      "3  | init      | InitialBlock           | 406   \n",
      "4  | b10       | RDDNeck                | 605   \n",
      "5  | b11       | RDDNeck                | 4.5 K \n",
      "6  | b12       | RDDNeck                | 4.5 K \n",
      "7  | b13       | RDDNeck                | 4.5 K \n",
      "8  | b14       | RDDNeck                | 4.5 K \n",
      "9  | b20       | RDDNeck                | 5.7 K \n",
      "10 | b21       | RDDNeck                | 17.8 K\n",
      "11 | b22       | RDDNeck                | 17.8 K\n",
      "12 | b23       | ASNeck                 | 18.8 K\n",
      "13 | b24       | RDDNeck                | 17.8 K\n",
      "14 | b25       | RDDNeck                | 17.8 K\n",
      "15 | b26       | RDDNeck                | 17.8 K\n",
      "16 | b27       | ASNeck                 | 18.8 K\n",
      "17 | b28       | RDDNeck                | 17.8 K\n",
      "18 | b31       | RDDNeck                | 17.8 K\n",
      "19 | b32       | RDDNeck                | 17.8 K\n",
      "20 | b33       | ASNeck                 | 18.8 K\n",
      "21 | b34       | RDDNeck                | 17.8 K\n",
      "22 | b35       | RDDNeck                | 17.8 K\n",
      "23 | b36       | RDDNeck                | 17.8 K\n",
      "24 | b37       | ASNeck                 | 18.8 K\n",
      "25 | b38       | RDDNeck                | 17.8 K\n",
      "26 | b40       | UBNeck                 | 23.8 K\n",
      "27 | b41       | RDDNeck                | 4.5 K \n",
      "28 | b42       | RDDNeck                | 4.5 K \n",
      "29 | b50       | UBNeck                 | 4.7 K \n",
      "30 | b51       | RDDNeck                | 316   \n",
      "31 | fullconv  | ConvTranspose2d        | 144   \n",
      "------------------------------------------------------\n",
      "350 K     Trainable params\n",
      "0         Non-trainable params\n",
      "350 K     Total params\n",
      "1.404     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  9.12it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\2073484368.py\", line 30, in train_using_wandb\n",
      "    trainer.fit(model, datamodule=idd_data)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1030, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1059, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py\", line 182, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 142, in run\n",
      "    return self.on_run_end()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 254, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 334, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\4263242126.py\", line 20, in on_validation_epoch_end\n",
      "    dummy_input = torch.zeros_like(self.img_aug_func_train, device = self.device)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "AttributeError: 'joined_ENet' object has no attribute 'img_aug_func_train'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">icy-sweep-6</strong> at: <a href='https://wandb.ai/dayaalex/try3/runs/pnz4n8de' target=\"_blank\">https://wandb.ai/dayaalex/try3/runs/pnz4n8de</a><br/>Synced 5 W&B file(s), 1 media file(s), 7 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240228_045322-pnz4n8de\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run pnz4n8de errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\2073484368.py\", line 30, in train_using_wandb\n",
      "    trainer.fit(model, datamodule=idd_data)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1030, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1059, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py\", line 182, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 142, in run\n",
      "    return self.on_run_end()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 254, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 334, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\4263242126.py\", line 20, in on_validation_epoch_end\n",
      "    dummy_input = torch.zeros_like(self.img_aug_func_train, device = self.device)\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\n",
      "    raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "AttributeError: 'joined_ENet' object has no attribute 'img_aug_func_train'\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run pnz4n8de errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\2073484368.py\", line 30, in train_using_wandb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, datamodule=idd_data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1030, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1059, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\utilities.py\", line 182, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 142, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.on_run_end()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 254, in on_run_end\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._on_evaluation_epoch_end()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\evaluation_loop.py\", line 334, in _on_evaluation_epoch_end\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_lightning_module_hook(trainer, hook_name)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_33700\\4263242126.py\", line 20, in on_validation_epoch_end\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     dummy_input = torch.zeros_like(self.img_aug_func_train, device = self.device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                    ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1695, in __getattr__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{name}'\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m AttributeError: 'joined_ENet' object has no attribute 'img_aug_func_train'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Detected 3 failed runs in the first 60 seconds, killing sweep.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true\n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train_using_wandb, count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLw_Y8emp9HM"
   },
   "source": [
    "Define the function that maps a 2D image with all the class labels to a segmented image with the specified colored maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkyoipIGZKk9"
   },
   "outputs": [],
   "source": [
    "def decode_segmap(image):\n",
    "    Sky = [128, 128, 128]\n",
    "    Building = [128, 0, 0]\n",
    "    Pole = [192, 192, 128]\n",
    "    Road_marking = [255, 69, 0]\n",
    "    Road = [128, 64, 128]\n",
    "    Pavement = [60, 40, 222]\n",
    "    Tree = [128, 128, 0]\n",
    "    SignSymbol = [192, 128, 128]\n",
    "    Fence = [64, 64, 128]\n",
    "    Car = [64, 0, 128]\n",
    "    Pedestrian = [64, 64, 0]\n",
    "    Bicyclist = [0, 128, 192]\n",
    "\n",
    "    label_colours = np.array([Sky, Building, Pole, Road_marking, Road, \n",
    "                              Pavement, Tree, SignSymbol, Fence, Car, \n",
    "                              Pedestrian, Bicyclist]).astype(np.uint8)\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    for l in range(0, 12):\n",
    "        r[image == l] = label_colours[l, 0]\n",
    "        g[image == l] = label_colours[l, 1]\n",
    "        b[image == l] = label_colours[l, 2]\n",
    "\n",
    "    rgb = np.zeros((image.shape[0], image.shape[1], 3)).astype(np.uint8)\n",
    "    rgb[:, :, 0] = b\n",
    "    rgb[:, :, 1] = g\n",
    "    rgb[:, :, 2] = r\n",
    "    return rgb"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ENet - Real Time Semantic Segmentation.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "daya_pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
