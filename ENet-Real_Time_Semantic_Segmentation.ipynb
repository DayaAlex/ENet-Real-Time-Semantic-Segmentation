{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8qnyf_pzhKXv"
   },
   "source": [
    "## Install the dependencies and Import them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5NCTHdEqj317"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 3556773588\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "pl.seed_everything(hash(\"setting random seeds\") % 2**32 - 1)\n",
    "\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from torchmetrics import JaccardIndex\n",
    "import albumentations as A\n",
    "from segmentation_models_pytorch.losses import DiceLoss\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import glob\n",
    "\n",
    "#from torch.optim.lr_scheduler import StepLR\n",
    "#import torch.optim.lr_scheduler as lr_scheduler\n",
    "from PIL import Image\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WANDB SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_NOTEBOOK_NAME = 'Enet_idd_lite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wandb sweep setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "    }\n",
    "\n",
    "metric = {\n",
    "    'name': 'loss',\n",
    "    'goal': 'minimize',\n",
    "    'name': 'IoU',\n",
    "    'goal': 'maximize'\n",
    "    }\n",
    "\n",
    "sweep_config['metric'] = metric\n",
    "\n",
    "parameters_dict = {\n",
    "    'optimizer': {\n",
    "        'values': ['adam', 'sgd']\n",
    "        },\n",
    "    'batch_size':{\n",
    "        'values':[5,10,15,20]\n",
    "    },\n",
    "    'learning_rate': {\n",
    "    'distribution': 'log_uniform_values',\n",
    "    'min': 5e-6,\n",
    "    'max': 5e-2\n",
    "    },\n",
    "    'image_ip_size':{\n",
    "        'values': [224,512]\n",
    "    }\n",
    "    }\n",
    "\n",
    "parameters_dict.update({\n",
    "    'epochs':{\n",
    "        'value': 5\n",
    "    }\n",
    "})\n",
    "\n",
    "sweep_config['parameters'] = parameters_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'metric': {'goal': 'maximize', 'name': 'IoU'},\n",
      " 'parameters': {'batch_size': {'values': [5, 10, 15, 20]},\n",
      "                'epochs': {'value': 5},\n",
      "                'image_ip_size': {'values': [224, 512]},\n",
      "                'learning_rate': {'distribution': 'log_uniform_values',\n",
      "                                  'max': 0.05,\n",
      "                                  'min': 5e-06},\n",
      "                'optimizer': {'values': ['adam', 'sgd']}}}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 1u7j7420\n",
      "Sweep URL: https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/sweeps/1u7j7420\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project ='testing_wandb_sweep_on_IDD_lite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET SECTION "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Augmentation(mode='train', h=224, w=224 ):\n",
    "    if mode == 'train':\n",
    "        img_transformation = A.Compose([\n",
    "                        A.Resize(h,w),\n",
    "                        A.HorizontalFlip(p= 0.5),\n",
    "                        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                    ])\n",
    "        mask_transformation = A.Compose([\n",
    "                    A.Resize(h, w),\n",
    "                    A.HorizontalFlip(p=0.5)\n",
    "                    ])\n",
    "        \n",
    "    elif mode == 'val':\n",
    "        img_transformation =A.Resize(h, w)\n",
    "        mask_transformation = A.Resize(h, w)\n",
    "       \n",
    "    elif mode == 'test':\n",
    "        img_transformation = A.Resize(h, w)\n",
    "        mask_transformation = A.Resize(h, w)\n",
    "\n",
    "    return img_transformation,  mask_transformation\n",
    "\n",
    "class DualTransform:\n",
    "    def __init__(self, image_transform, mask_transform):\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __call__(self, img_and_mask):\n",
    "        img, mask = img_and_mask\n",
    "        # Apply the same random transformation to both image and mask\n",
    "        seed = random.randint(0, 2**32)\n",
    "        random.seed(seed)\n",
    "        transformed_img = self.image_transform(image=np.array(img))['image']\n",
    "        random.seed(seed)\n",
    "        transformed_mask = self.mask_transform(image=np.array(mask))['image']\n",
    "        return transformed_img, transformed_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_dataset_normalization(loader):\n",
    "    # Initialize accumulators\n",
    "    channel_sum = torch.tensor([0.0, 0.0, 0.0])\n",
    "    channel_squared_sum = torch.tensor([0.0, 0.0, 0.0])\n",
    "    num_batches = 0\n",
    "\n",
    "    for images, _ in loader:\n",
    "        # Accumulate sum and squared sum for each channel\n",
    "        channel_sum += images.sum(dim=[0, 2, 3])\n",
    "        channel_squared_sum += (images ** 2).sum(dim=[0, 2, 3])\n",
    "        num_batches += images.shape[0]\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    mean = channel_sum / (num_batches *224  *224 )\n",
    "    std = (channel_squared_sum / (num_batches * 224 *224 ) - mean ** 2) ** 0.5\n",
    "\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, training_path, segmented_path,road_idx, norm_transform=None, dual_transform=None):\n",
    "        #\"C:\\Users\\dalex\\Desktop\\Daya\\datasets\\idd-lite\\idd20k_lite\\leftImg8bit\\train\\2\\593144_image.jpg\"\n",
    "        #print('paths',training_path,segmented_path)\n",
    "        \n",
    "        self.filenames_t = glob.glob(training_path+f'*\\*.jpg')\n",
    "        #print(self.filenames_t)\n",
    "        self.filenames_s = glob.glob(segmented_path+f'*\\*_label.png')\n",
    "        #print(self.filenames_s)\n",
    "        self.norm_transform = norm_transform\n",
    "        self.dual_transform = dual_transform\n",
    "        self.road_idx = road_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames_t)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.filenames_t[idx]\n",
    "        mask_path = self.filenames_s[idx]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        label_array = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        road_mask = np.zeros_like(label_array)\n",
    "        road_mask[label_array == self.road_idx] = 1\n",
    "        road_mask = np.expand_dims(road_mask,axis = -1)\n",
    "\n",
    "        if self.dual_transform:\n",
    "            img, mask = self.dual_transform((img, road_mask))\n",
    "\n",
    "        if self.norm_transform:\n",
    "            img = self.norm_transform(img)\n",
    "            \n",
    "        if not isinstance(mask, torch.Tensor):\n",
    "            mask = torch.tensor(mask, dtype=torch.uint8)\n",
    "        mask = mask.permute(2, 0, 1)\n",
    "        \n",
    "        return img, mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class idd_lite_datamodule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self,image_ip_size,batch_size):\n",
    "        super().__init__()\n",
    "        self.image_ip_size = image_ip_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        \n",
    "        img_aug_func, mask_aug_func = Augmentation('train',h = self.image_ip_size, w =self.image_ip_size )\n",
    "\n",
    "        dual_transform = DualTransform(img_aug_func, mask_aug_func)\n",
    "\n",
    "\n",
    "        transform_norm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.3576, 0.3713, 0.3657], std=[0.2607, 0.2723, 0.2943])\n",
    "        ])\n",
    "\n",
    "        # Instantiate your dataset\n",
    "        self.train_dataset = CustomDataset(\"c:\\\\Users\\\\dalex\\\\Desktop\\\\Daya\\\\datasets\\\\idd-lite\\\\idd20k_lite\\\\leftImg8bit\\\\train\\\\\",\n",
    "                                    \"c:\\\\Users\\\\dalex\\\\Desktop\\\\Daya\\\\datasets\\\\idd-lite\\\\idd20k_lite\\\\gtFine\\\\train\\\\\", \n",
    "                                    road_idx=0,\n",
    "                                    norm_transform=transform_norm,\n",
    "                                    dual_transform=dual_transform)\n",
    "\n",
    "        \n",
    "        tr_data_loader  = DataLoader(self.train_dataset, batch_size= self.batch_size, shuffle=True)\n",
    "\n",
    "        return tr_data_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        img_aug_func, mask_aug_func = Augmentation('val', h = self.image_ip_size, w = self.image_ip_size)\n",
    "\n",
    "        dual_transform = DualTransform(img_aug_func, mask_aug_func)\n",
    "\n",
    "        transform_norm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.3576, 0.3713, 0.3657], std=[0.2607, 0.2723, 0.2943])\n",
    "        ])\n",
    "\n",
    "        # Instantiate your dataset\n",
    "        self.val_dataset = CustomDataset(\"c:\\\\Users\\\\dalex\\\\Desktop\\\\Daya\\\\datasets\\\\idd-lite\\\\idd20k_lite\\\\leftImg8bit\\\\val\\\\\",\n",
    "                                    \"c:\\\\Users\\\\dalex\\\\Desktop\\\\Daya\\\\datasets\\\\idd-lite\\\\idd20k_lite\\\\gtFine\\\\val\\\\\", \n",
    "                                    road_idx=0,\n",
    "                                    norm_transform=transform_norm,\n",
    "                                    dual_transform=dual_transform)\n",
    "        \n",
    "        val_data_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False) \n",
    "\n",
    "        return val_data_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if there is any gpu available and pass the model to gpu or cpu\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i26TZVXmhewY"
   },
   "source": [
    "## MODEL DEFINITIONS\n",
    "\n",
    "trying out variations between joined training and seperated training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kqHUezLfPBwn"
   },
   "outputs": [],
   "source": [
    "class InitialBlock(nn.Module):\n",
    "  \n",
    "  # Initial block of the model:\n",
    "  #         Input\n",
    "  #        /     \\\n",
    "  #       /       \\\n",
    "  #maxpool2d    conv2d-3x3\n",
    "  #       \\       /  \n",
    "  #        \\     /\n",
    "  #      concatenate\n",
    "   \n",
    "    def __init__ (self,in_channels = 3,out_channels = 13):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, \n",
    "                                      stride = 2, \n",
    "                                      padding = 0)\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels, \n",
    "                                out_channels,\n",
    "                                kernel_size = 3,\n",
    "                                stride = 2, \n",
    "                                padding = 1)\n",
    "\n",
    "        self.prelu = nn.PReLU(16)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "  \n",
    "    def forward(self, x):\n",
    "        \n",
    "        main = self.conv(x)\n",
    "        main = self.batchnorm(main)\n",
    "        \n",
    "        side = self.maxpool(x)\n",
    "        \n",
    "        # concatenating on the channels axis\n",
    "        x = torch.cat((main, side), dim=1)\n",
    "        x = self.prelu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERqXRpl_sfSE"
   },
   "outputs": [],
   "source": [
    "class UBNeck(nn.Module):\n",
    "    \n",
    "  # Upsampling bottleneck:\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  # conv2d-1x1     convTrans2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         convTrans2d-3x3\n",
    "  #      |             | PReLU\n",
    "  #      |         convTrans2d-1x1\n",
    "  #      |             |\n",
    "  # maxunpool2d    Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  #  Params: \n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
    "  \n",
    "    def __init__(self, in_channels, out_channels, relu=False, projection_ratio=4):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        self.reduced_depth = int(in_channels / projection_ratio)\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        \n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "        \n",
    "        self.unpool = nn.MaxUnpool2d(kernel_size = 2,\n",
    "                                     stride = 2)\n",
    "        \n",
    "        self.main_conv = nn.Conv2d(in_channels = self.in_channels,\n",
    "                                    out_channels = self.out_channels,\n",
    "                                    kernel_size = 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        \n",
    "        \n",
    "        self.convt1 = nn.ConvTranspose2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False)\n",
    "        \n",
    "        \n",
    "        self.prelu1 = activation\n",
    "        \n",
    "        # This layer used for Upsampling\n",
    "        self.convt2 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = 2,\n",
    "                                  padding = 1,\n",
    "                                  output_padding = 1,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu2 = activation\n",
    "        \n",
    "        self.convt3 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu3 = activation\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "    def forward(self, x, indices):\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.convt1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.convt2(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.convt3(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        \n",
    "        x_copy = self.main_conv(x_copy)\n",
    "        x_copy = self.unpool(x_copy, indices, output_size=x.size())\n",
    "        \n",
    "        # summing the main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-nTIAS9bd9z"
   },
   "outputs": [],
   "source": [
    "class RDDNeck(nn.Module):\n",
    "    def __init__(self, dilation, in_channels, out_channels, down_flag, relu=False, projection_ratio=4, p=0.1):\n",
    "      \n",
    "  # Regular|Dilated|Downsampling bottlenecks:\n",
    "  #\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  # maxpooling2d   conv2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-3x3\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             |\n",
    "  #  Padding2d     Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  # Params: \n",
    "  #  dilation (bool) - if True: creating dilation bottleneck\n",
    "  #  down_flag (bool) - if True: creating downsampling bottleneck\n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
    "  #  p - dropout ratio\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "        self.dilation = dilation\n",
    "        self.down_flag = down_flag\n",
    "        \n",
    "        # calculating the number of reduced channels\n",
    "        if down_flag:\n",
    "            self.stride = 2\n",
    "            self.reduced_depth = int(in_channels // projection_ratio)\n",
    "        else:\n",
    "            self.stride = 1\n",
    "            self.reduced_depth = int(out_channels // projection_ratio)\n",
    "        \n",
    "        if relu:\n",
    "            activation = nn.ReLU()\n",
    "        else:\n",
    "            activation = nn.PReLU()\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2,\n",
    "                                      stride = 2,\n",
    "                                      padding = 0, return_indices=True)\n",
    "        \n",
    "\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=p)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False,\n",
    "                               dilation = 1)\n",
    "        \n",
    "        self.prelu1 = activation\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = 3,\n",
    "                                  stride = self.stride,\n",
    "                                  padding = self.dilation,\n",
    "                                  bias = True,\n",
    "                                  dilation = self.dilation)\n",
    "                                  \n",
    "        self.prelu2 = activation\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  stride = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False,\n",
    "                                  dilation = 1)\n",
    "        \n",
    "        self.prelu3 = activation\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        bs = x.size()[0]\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm2(x)\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        if self.down_flag:\n",
    "            x_copy, indices = self.maxpool(x_copy)\n",
    "          \n",
    "        if self.in_channels != self.out_channels:\n",
    "            out_shape = self.out_channels - self.in_channels\n",
    "            \n",
    "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
    "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
    "            extras = extras.to(device)\n",
    "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
    "\n",
    "        # Summing main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        if self.down_flag:\n",
    "            return x, indices\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tb_i1sCvtmMF"
   },
   "outputs": [],
   "source": [
    "class ASNeck(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, projection_ratio=4):\n",
    "      \n",
    "  # Asymetric bottleneck:\n",
    "  #\n",
    "  #     Bottleneck Input\n",
    "  #        /        \\\n",
    "  #       /          \\\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x5\n",
    "  #      |             |\n",
    "  #      |         conv2d-5x1\n",
    "  #      |             | PReLU\n",
    "  #      |         conv2d-1x1\n",
    "  #      |             |\n",
    "  #  Padding2d     Regularizer\n",
    "  #       \\           /  \n",
    "  #        \\         /\n",
    "  #      Summing + PReLU\n",
    "  #\n",
    "  # Params:    \n",
    "  #  projection_ratio - ratio between input and output channels\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        self.in_channels = in_channels\n",
    "        self.reduced_depth = int(in_channels / projection_ratio)\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
    "                               out_channels = self.reduced_depth,\n",
    "                               kernel_size = 1,\n",
    "                               stride = 1,\n",
    "                               padding = 0,\n",
    "                               bias = False)\n",
    "        \n",
    "        self.prelu1 = nn.PReLU()\n",
    "        \n",
    "        self.conv21 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = (1, 5),\n",
    "                                  stride = 1,\n",
    "                                  padding = (0, 2),\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.conv22 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.reduced_depth,\n",
    "                                  kernel_size = (5, 1),\n",
    "                                  stride = 1,\n",
    "                                  padding = (2, 0),\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu2 = nn.PReLU()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
    "                                  out_channels = self.out_channels,\n",
    "                                  kernel_size = 1,\n",
    "                                  stride = 1,\n",
    "                                  padding = 0,\n",
    "                                  bias = False)\n",
    "        \n",
    "        self.prelu3 = nn.PReLU()\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs = x.size()[0]\n",
    "        x_copy = x\n",
    "        \n",
    "        # Side Branch\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu1(x)\n",
    "        \n",
    "        x = self.conv21(x)\n",
    "        x = self.conv22(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.prelu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "                \n",
    "        x = self.dropout(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        \n",
    "        # Main Branch\n",
    "        \n",
    "        if self.in_channels != self.out_channels:\n",
    "            out_shape = self.out_channels - self.in_channels\n",
    "            \n",
    "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
    "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
    "            extras = extras.to(x_copy.device)\n",
    "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
    "        \n",
    "        # Summing main and side branches\n",
    "        x = x + x_copy\n",
    "        x = self.prelu3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z1pz_bve690y"
   },
   "outputs": [],
   "source": [
    "class joined_ENet(pl.LightningModule):\n",
    "  \n",
    "  # Creating Enet model!\n",
    "  \n",
    "    def __init__(self, lr,optim):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = 1\n",
    "        self.lr = lr\n",
    "        self.optimizer_name = optim\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_acc = JaccardIndex(task = 'multiclass', num_classes = 2)\n",
    "        self.valid_acc = JaccardIndex(task = 'multiclass', num_classes = 2)\n",
    "\n",
    "        # The initial block\n",
    "        self.init = InitialBlock()\n",
    "        \n",
    "        \n",
    "        # The first bottleneck\n",
    "        self.b10 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=64, \n",
    "                           down_flag=True, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b11 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b12 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b13 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b14 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        \n",
    "        # The second bottleneck\n",
    "        self.b20 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=128, \n",
    "                           down_flag=True)\n",
    "        \n",
    "        self.b21 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b22 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b23 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b24 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b25 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b26 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b27 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b28 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The third bottleneck\n",
    "        self.b31 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b32 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b33 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b34 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b35 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b36 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b37 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b38 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        self.b40 = UBNeck(in_channels=128, \n",
    "                          out_channels=64, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b41 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        self.b42 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        self.b50 = UBNeck(in_channels=64, \n",
    "                          out_channels=16, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b51 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=16, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n",
    "                                           out_channels=self.C, \n",
    "                                           kernel_size=3, \n",
    "                                           stride=2, \n",
    "                                           padding=1, \n",
    "                                           output_padding=1,\n",
    "                                           bias=False)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # The initial block\n",
    "        x = self.init(x)\n",
    "        \n",
    "        # The first bottleneck\n",
    "        x, i1 = self.b10(x)\n",
    "        x = self.b11(x)\n",
    "        x = self.b12(x)\n",
    "        x = self.b13(x)\n",
    "        x = self.b14(x)\n",
    "        \n",
    "        # The second bottleneck\n",
    "        x, i2 = self.b20(x)\n",
    "        x = self.b21(x)\n",
    "        x = self.b22(x)\n",
    "        x = self.b23(x)\n",
    "        x = self.b24(x)\n",
    "        x = self.b25(x)\n",
    "        x = self.b26(x)\n",
    "        x = self.b27(x)\n",
    "        x = self.b28(x)\n",
    "        \n",
    "        # The third bottleneck\n",
    "        x = self.b31(x)\n",
    "        x = self.b32(x)\n",
    "        x = self.b33(x)\n",
    "        x = self.b34(x)\n",
    "        x = self.b35(x)\n",
    "        x = self.b36(x)\n",
    "        x = self.b37(x)\n",
    "        x = self.b38(x)\n",
    "        \n",
    "        # The fourth bottleneck\n",
    "        x = self.b40(x, i2)\n",
    "        x = self.b41(x)\n",
    "        x = self.b42(x)\n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        x = self.b50(x, i1)\n",
    "        x = self.b51(x)\n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        x = self.fullconv(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def loss(self, inputs, target):\n",
    "        if target.dtype == torch.uint8:\n",
    "            target = target.float()\n",
    "\n",
    "        pred = self(inputs) \n",
    "        loss1 = DiceLoss(mode='binary')(pred, target)\n",
    "        loss2 = nn.BCEWithLogitsLoss()(pred, target)\n",
    "        return loss1 + loss2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(pl.LightningModule):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "        \n",
    "        # The initial block\n",
    "        self.init_layer = InitialBlock()\n",
    "        \n",
    "        \n",
    "        # The first bottleneck\n",
    "        self.b10 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=64, \n",
    "                           down_flag=True, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b11 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b12 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b13 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        self.b14 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           p=0.01)\n",
    "        \n",
    "        \n",
    "        # The second bottleneck\n",
    "        self.b20 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=128, \n",
    "                           down_flag=True)\n",
    "        \n",
    "        self.b21 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b22 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b23 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b24 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b25 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b26 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b27 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b28 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        \n",
    "        # The third bottleneck\n",
    "        self.b31 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b32 = RDDNeck(dilation=2, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b33 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b34 = RDDNeck(dilation=4, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b35 = RDDNeck(dilation=1, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b36 = RDDNeck(dilation=8, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "        self.b37 = ASNeck(in_channels=128, \n",
    "                          out_channels=128)\n",
    "        \n",
    "        self.b38 = RDDNeck(dilation=16, \n",
    "                           in_channels=128, \n",
    "                           out_channels=128, \n",
    "                           down_flag=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # The initial block\n",
    "        x = self.init_layer(x)\n",
    "        \n",
    "        # The first bottleneck\n",
    "        x, i1 = self.b10(x)\n",
    "        x = self.b11(x)\n",
    "        x = self.b12(x)\n",
    "        x = self.b13(x)\n",
    "        x = self.b14(x)\n",
    "        \n",
    "        # The second bottleneck\n",
    "        x, i2 = self.b20(x)\n",
    "        x = self.b21(x)\n",
    "        x = self.b22(x)\n",
    "        x = self.b23(x)\n",
    "        x = self.b24(x)\n",
    "        x = self.b25(x)\n",
    "        x = self.b26(x)\n",
    "        x = self.b27(x)\n",
    "        x = self.b28(x)\n",
    "        \n",
    "        # The third bottleneck\n",
    "        x = self.b31(x)\n",
    "        x = self.b32(x)\n",
    "        x = self.b33(x)\n",
    "        x = self.b34(x)\n",
    "        x = self.b35(x)\n",
    "        x = self.b36(x)\n",
    "        x = self.b37(x)\n",
    "        x = self.b38(x)\n",
    "\n",
    "        return x, i1,i2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class decoder(pl.LightningModule):\n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define class variables\n",
    "        # C - number of classes\n",
    "        self.C = C\n",
    "\n",
    "\n",
    "        # The fourth bottleneck\n",
    "        self.b40 = UBNeck(in_channels=128, \n",
    "                          out_channels=64, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b41 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        self.b42 = RDDNeck(dilation=1, \n",
    "                           in_channels=64, \n",
    "                           out_channels=64, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        self.b50 = UBNeck(in_channels=64, \n",
    "                          out_channels=16, \n",
    "                          relu=True)\n",
    "        \n",
    "        self.b51 = RDDNeck(dilation=1, \n",
    "                           in_channels=16, \n",
    "                           out_channels=16, \n",
    "                           down_flag=False, \n",
    "                           relu=True)\n",
    "        \n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n",
    "                                           out_channels=self.C, \n",
    "                                           kernel_size=3, \n",
    "                                           stride=2, \n",
    "                                           padding=1, \n",
    "                                           output_padding=1,\n",
    "                                           bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x, i1, i2):\n",
    "        # The fourth bottleneck\n",
    "        x = self.b40(x, i2)\n",
    "        x = self.b41(x)\n",
    "        x = self.b42(x)\n",
    "        \n",
    "        # The fifth bottleneck\n",
    "        x = self.b50(x, i1)\n",
    "        x = self.b51(x)\n",
    "        \n",
    "        # Final ConvTranspose Layer\n",
    "        x = self.fullconv(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCODER-DECODER\n",
    "class enc_dec_ENet(pl.LightningModule):\n",
    "  \n",
    "  # Creating Enet model!\n",
    "  \n",
    "    def __init__(self, C):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder(C)\n",
    "        \n",
    "        # Create the decoder\n",
    "        self.decoder = decoder(C)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.train_acc = JaccardIndex(task = 'multiclass', num_classes = 2)\n",
    "        self.valid_acc = JaccardIndex(task = 'multiclass', num_classes = 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the encoder\n",
    "        encoder_output, i1, i2 = self.encoder(x)\n",
    "\n",
    "        # Pass encoder output and intermediate feature maps to the decoder\n",
    "        decoder_output = self.decoder(encoder_output, i1, i2)\n",
    "    \n",
    "        return decoder_output\n",
    "\n",
    "    def combined_loss(self, inputs, target):\n",
    "        if target.dtype == torch.uint8:\n",
    "            target = target.float()\n",
    "\n",
    "        pred = self(inputs) \n",
    "        loss1 = DiceLoss(mode='binary')(pred, target)\n",
    "        loss2 = nn.BCEWithLogitsLoss()(pred, target)\n",
    "        return (loss1 + loss2), pred\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizers(self):\n",
    "    if self.optimizer_name =='adam':\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=2e-4)\n",
    "    elif self.optimizer_name == 'sgd':\n",
    "        optimizer = torch.optim.SGD(self.parameters(),\n",
    "                                lr=self.lr, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {self.optimizer_name}\")\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFIdlVWviBYl"
   },
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "WQ6XJzl6Ta1_",
    "outputId": "a3f62522-391d-4e05-f138-11c78a0d90cf"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train loop\n",
    "def training_step(self,batch, batch_idx):\n",
    "\n",
    "    X_batch, mask_batch = batch\n",
    "    net_loss, preds = self.loss(X_batch, mask_batch)#mask_batch.long??\n",
    "    self.train_acc(preds,mask_batch)\n",
    "\n",
    "    self.log('train-loss: ', net_loss, on_epoch = True )\n",
    "    self.log('train-iou: ', self.train_acc, on_epoch = True)\n",
    "\n",
    "    return net_loss#loss must be returned to facilitate grad calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_using_wandb(config = None):\n",
    "    with wandb.init(config = config):\n",
    "        config = wandb.config# automatically set by sweepcontroller previously configuired\n",
    "        logger = WandbLogger(project='testing_wandb_sweep_on_IDD_lite')\n",
    "\n",
    "        idd_data = idd_lite_datamodule(config.image_ip_size,config.batch_size)\n",
    "        joined_ENet.training_step = training_step\n",
    "        joined_ENet.configure_optimizers = configure_optimizers\n",
    "        model = joined_ENet(config.learning_rate, config.optimizer)        \n",
    "        \n",
    "        trainer = pl.Trainer(\n",
    "        logger = logger,\n",
    "        log_every_n_steps = 50,\n",
    "        max_epochs = config.epochs,\n",
    "        deterministic = False\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, idd_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: tdu5uys7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_ip_size: 224\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.9593135694922397e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\wandb\\run-20240215_103436-tdu5uys7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/runs/tdu5uys7' target=\"_blank\">fanciful-sweep-1</a></strong> to <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/sweeps/1u7j7420' target=\"_blank\">https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/sweeps/1u7j7420</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite' target=\"_blank\">https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/sweeps/1u7j7420' target=\"_blank\">https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/sweeps/1u7j7420</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/runs/tdu5uys7' target=\"_blank\">https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/runs/tdu5uys7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type                   | Params\n",
      "------------------------------------------------------\n",
      "0  | train_acc | MulticlassJaccardIndex | 0     \n",
      "1  | valid_acc | MulticlassJaccardIndex | 0     \n",
      "2  | init      | InitialBlock           | 406   \n",
      "3  | b10       | RDDNeck                | 605   \n",
      "4  | b11       | RDDNeck                | 4.5 K \n",
      "5  | b12       | RDDNeck                | 4.5 K \n",
      "6  | b13       | RDDNeck                | 4.5 K \n",
      "7  | b14       | RDDNeck                | 4.5 K \n",
      "8  | b20       | RDDNeck                | 5.7 K \n",
      "9  | b21       | RDDNeck                | 17.8 K\n",
      "10 | b22       | RDDNeck                | 17.8 K\n",
      "11 | b23       | ASNeck                 | 18.8 K\n",
      "12 | b24       | RDDNeck                | 17.8 K\n",
      "13 | b25       | RDDNeck                | 17.8 K\n",
      "14 | b26       | RDDNeck                | 17.8 K\n",
      "15 | b27       | ASNeck                 | 18.8 K\n",
      "16 | b28       | RDDNeck                | 17.8 K\n",
      "17 | b31       | RDDNeck                | 17.8 K\n",
      "18 | b32       | RDDNeck                | 17.8 K\n",
      "19 | b33       | ASNeck                 | 18.8 K\n",
      "20 | b34       | RDDNeck                | 17.8 K\n",
      "21 | b35       | RDDNeck                | 17.8 K\n",
      "22 | b36       | RDDNeck                | 17.8 K\n",
      "23 | b37       | ASNeck                 | 18.8 K\n",
      "24 | b38       | RDDNeck                | 17.8 K\n",
      "25 | b40       | UBNeck                 | 23.8 K\n",
      "26 | b41       | RDDNeck                | 4.5 K \n",
      "27 | b42       | RDDNeck                | 4.5 K \n",
      "28 | b50       | UBNeck                 | 4.7 K \n",
      "29 | b51       | RDDNeck                | 316   \n",
      "30 | fullconv  | ConvTranspose2d        | 144   \n",
      "------------------------------------------------------\n",
      "350 K     Trainable params\n",
      "0         Non-trainable params\n",
      "350 K     Total params\n",
      "1.404     Total estimated model params size (MB)\n",
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/71 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\1599953307.py\", line 18, in train_using_wandb\n",
      "    trainer.fit(model, idd_data)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1032, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 138, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 242, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 191, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 269, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py\", line 1303, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py\", line 152, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py\", line 239, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py\", line 122, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 373, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\optim\\sgd.py\", line 66, in step\n",
      "    loss = closure()\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py\", line 108, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 144, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 129, in closure\n",
      "    step_output = self._step_fn()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 319, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py\", line 391, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\2937534734.py\", line 5, in training_step\n",
      "    net_loss, preds = self.loss(X_batch, mask_batch)#mask_batch.long??\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\2974868170.py\", line 225, in loss\n",
      "    target = target.float()\n",
      "             ^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 12.00 GiB of which 0 bytes is free. Of the allocated memory 17.41 GiB is allocated by PyTorch, and 124.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fanciful-sweep-1</strong> at: <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/runs/tdu5uys7' target=\"_blank\">https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/runs/tdu5uys7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240215_103436-tdu5uys7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run tdu5uys7 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\1599953307.py\", line 18, in train_using_wandb\n",
      "    trainer.fit(model, idd_data)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1032, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 138, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 242, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 191, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 269, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py\", line 1303, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py\", line 152, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py\", line 239, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py\", line 122, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 373, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\optim\\sgd.py\", line 66, in step\n",
      "    loss = closure()\n",
      "           ^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py\", line 108, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 144, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 129, in closure\n",
      "    step_output = self._step_fn()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 319, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py\", line 391, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\2937534734.py\", line 5, in training_step\n",
      "    net_loss, preds = self.loss(X_batch, mask_batch)#mask_batch.long??\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\2974868170.py\", line 225, in loss\n",
      "    target = target.float()\n",
      "             ^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 12.00 GiB of which 0 bytes is free. Of the allocated memory 17.41 GiB is allocated by PyTorch, and 124.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run tdu5uys7 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\1599953307.py\", line 18, in train_using_wandb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, idd_data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1032, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.fit_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 205, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 363, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.epoch_loop.run(self._data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 138, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance(data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 242, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 191, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._optimizer_step(batch_idx, closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 269, in _optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_lightning_module_hook(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py\", line 1303, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py\", line 152, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py\", line 239, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py\", line 122, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return optimizer.step(closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 373, in wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     out = func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\optim\\optimizer.py\", line 76, in _use_grad\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ret = func(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\optim\\sgd.py\", line 66, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py\", line 108, in _wrap_closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     closure_result = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                      ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 144, in __call__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._result = self.closure(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 129, in closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     step_output = self._step_fn()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py\", line 319, in _training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 309, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py\", line 391, in training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.training_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\2937534734.py\", line 5, in training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     net_loss, preds = self.loss(X_batch, mask_batch)#mask_batch.long??\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\2974868170.py\", line 225, in loss\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     target = target.float()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacty of 12.00 GiB of which 0 bytes is free. Of the allocated memory 17.41 GiB is allocated by PyTorch, and 124.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n9qtm326 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timage_ip_size: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00032800946077684727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\wandb\\run-20240215_103639-n9qtm326</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/runs/n9qtm326' target=\"_blank\">decent-sweep-2</a></strong> to <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/sweeps/1u7j7420' target=\"_blank\">https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/sweeps/1u7j7420</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite' target=\"_blank\">https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/sweeps/1u7j7420' target=\"_blank\">https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/sweeps/1u7j7420</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/runs/n9qtm326' target=\"_blank\">https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/runs/n9qtm326</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loggers\\wandb.py:390: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name      | Type                   | Params\n",
      "------------------------------------------------------\n",
      "0  | train_acc | MulticlassJaccardIndex | 0     \n",
      "1  | valid_acc | MulticlassJaccardIndex | 0     \n",
      "2  | init      | InitialBlock           | 406   \n",
      "3  | b10       | RDDNeck                | 605   \n",
      "4  | b11       | RDDNeck                | 4.5 K \n",
      "5  | b12       | RDDNeck                | 4.5 K \n",
      "6  | b13       | RDDNeck                | 4.5 K \n",
      "7  | b14       | RDDNeck                | 4.5 K \n",
      "8  | b20       | RDDNeck                | 5.7 K \n",
      "9  | b21       | RDDNeck                | 17.8 K\n",
      "10 | b22       | RDDNeck                | 17.8 K\n",
      "11 | b23       | ASNeck                 | 18.8 K\n",
      "12 | b24       | RDDNeck                | 17.8 K\n",
      "13 | b25       | RDDNeck                | 17.8 K\n",
      "14 | b26       | RDDNeck                | 17.8 K\n",
      "15 | b27       | ASNeck                 | 18.8 K\n",
      "16 | b28       | RDDNeck                | 17.8 K\n",
      "17 | b31       | RDDNeck                | 17.8 K\n",
      "18 | b32       | RDDNeck                | 17.8 K\n",
      "19 | b33       | ASNeck                 | 18.8 K\n",
      "20 | b34       | RDDNeck                | 17.8 K\n",
      "21 | b35       | RDDNeck                | 17.8 K\n",
      "22 | b36       | RDDNeck                | 17.8 K\n",
      "23 | b37       | ASNeck                 | 18.8 K\n",
      "24 | b38       | RDDNeck                | 17.8 K\n",
      "25 | b40       | UBNeck                 | 23.8 K\n",
      "26 | b41       | RDDNeck                | 4.5 K \n",
      "27 | b42       | RDDNeck                | 4.5 K \n",
      "28 | b50       | UBNeck                 | 4.7 K \n",
      "29 | b51       | RDDNeck                | 316   \n",
      "30 | fullconv  | ConvTranspose2d        | 144   \n",
      "------------------------------------------------------\n",
      "350 K     Trainable params\n",
      "0         Non-trainable params\n",
      "350 K     Total params\n",
      "1.404     Total estimated model params size (MB)\n",
      "c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/71 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\1599953307.py\", line 18, in train_using_wandb\n",
      "    trainer.fit(model, idd_data)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1032, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 138, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 215, in advance\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=0)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py\", line 347, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py\", line 336, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\hooks.py\", line 613, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\fabric\\utilities\\apply_func.py\", line 102, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning_utilities\\core\\apply_func.py\", line 66, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning_utilities\\core\\apply_func.py\", line 66, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\fabric\\utilities\\apply_func.py\", line 96, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 12.00 GiB of which 0 bytes is free. Of the allocated memory 17.41 GiB is allocated by PyTorch, and 124.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-sweep-2</strong> at: <a href='https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/runs/n9qtm326' target=\"_blank\">https://wandb.ai/dayaalex/testing_wandb_sweep_on_IDD_lite/runs/n9qtm326</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240215_103639-n9qtm326\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run n9qtm326 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\1599953307.py\", line 18, in train_using_wandb\n",
      "    trainer.fit(model, idd_data)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1032, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 205, in run\n",
      "    self.advance()\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 363, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 138, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 215, in advance\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=0)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 309, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py\", line 347, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py\", line 336, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\hooks.py\", line 613, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\fabric\\utilities\\apply_func.py\", line 102, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning_utilities\\core\\apply_func.py\", line 66, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning_utilities\\core\\apply_func.py\", line 66, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\fabric\\utilities\\apply_func.py\", line 96, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 12.00 GiB of which 0 bytes is free. Of the allocated memory 17.41 GiB is allocated by PyTorch, and 124.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run n9qtm326 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\wandb\\agents\\pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"C:\\Users\\dalex\\AppData\\Local\\Temp\\ipykernel_22132\\1599953307.py\", line 18, in train_using_wandb\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, idd_data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 543, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 44, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 579, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 986, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py\", line 1032, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.fit_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 205, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py\", line 363, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.epoch_loop.run(self._data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 138, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance(data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py\", line 215, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=0)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 309, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py\", line 347, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py\", line 336, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py\", line 157, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\pytorch\\core\\hooks.py\", line 613, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\fabric\\utilities\\apply_func.py\", line 102, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning_utilities\\core\\apply_func.py\", line 66, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning_utilities\\core\\apply_func.py\", line 66, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"c:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\lightning\\fabric\\utilities\\apply_func.py\", line 96, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacty of 12.00 GiB of which 0 bytes is free. Of the allocated memory 17.41 GiB is allocated by PyTorch, and 124.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id, train_using_wandb, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T9CV4mHhKYZ5"
   },
   "source": [
    "Load the ENet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9dWM6D5Rl_Mq"
   },
   "source": [
    "## Use the code to infer on new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8pxajTHj5vNG"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/test/Seq05VD_f05100.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSeq05VD_f05100.png\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m tmg_ \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/content/test/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m tmg_ \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(tmg_, (\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m512\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mINTER_NEAREST)\n\u001b[0;32m      4\u001b[0m tmg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tmg_)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "File \u001b[1;32mc:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\matplotlib\\pyplot.py:2389\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   2385\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(matplotlib\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimread)\n\u001b[0;32m   2386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimread\u001b[39m(\n\u001b[0;32m   2387\u001b[0m         fname: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath \u001b[38;5;241m|\u001b[39m BinaryIO, \u001b[38;5;28mformat\u001b[39m: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2388\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\matplotlib\\image.py:1525\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parse\u001b[38;5;241m.\u001b[39murlparse(fname)\u001b[38;5;241m.\u001b[39mscheme) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1519\u001b[0m     \u001b[38;5;66;03m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[0;32m   1520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1521\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease open the URL for reading and pass the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult to Pillow, e.g. with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1523\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1524\u001b[0m         )\n\u001b[1;32m-> 1525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimg_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[0;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[0;32m   1527\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mPngImagePlugin\u001b[38;5;241m.\u001b[39mPngImageFile) \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[0;32m   1528\u001b[0m             pil_to_array(image))\n",
      "File \u001b[1;32mc:\\Users\\dalex\\Desktop\\Daya\\ENet-Real-Time-Semantic-Segmentation\\daya_pytorch_env\\Lib\\site-packages\\PIL\\ImageFile.py:125\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[1;34m(self, fp, filename)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodermaxblock \u001b[38;5;241m=\u001b[39m MAXBLOCK\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m# filename\u001b[39;00m\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/test/Seq05VD_f05100.png'"
     ]
    }
   ],
   "source": [
    "fname = 'Seq05VD_f05100.png'\n",
    "tmg_ = plt.imread('/content/test/' + fname)\n",
    "tmg_ = cv2.resize(tmg_, (512, 512), cv2.INTER_NEAREST)\n",
    "tmg = torch.tensor(tmg_).unsqueeze(0).float()\n",
    "tmg = tmg.transpose(2, 3).transpose(1, 2).to(device)\n",
    "\n",
    "enet.to(device)\n",
    "with torch.no_grad():\n",
    "    out1 = enet(tmg.float()).squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N8vh2eozpAAh"
   },
   "source": [
    "## Load the label image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8zQiSaeeo_rZ"
   },
   "outputs": [],
   "source": [
    "smg_ = Image.open('/content/testannot/' + fname)\n",
    "smg_ = cv2.resize(np.array(smg_), (512, 512), cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1EXLLTnYpF2D"
   },
   "source": [
    "## Move the output to cpu and convert it to numpy and see how each class looks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HjpnkPB6RKub"
   },
   "outputs": [],
   "source": [
    "out2 = out1.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "U-sVZCMU6vgF",
    "outputId": "5d9c186e-75e2-4414-c6e3-b788453dbad7"
   },
   "outputs": [],
   "source": [
    "mno = 8 # Should be between 0 - n-1 | where n is the number of classes\n",
    "\n",
    "figure = plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Input Image')\n",
    "plt.axis('off')\n",
    "plt.imshow(tmg_)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Output Image')\n",
    "plt.axis('off')\n",
    "plt.imshow(out2[mno, :, :])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-8yIfMvrpZkT"
   },
   "source": [
    "Get the class labels from the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e1C9bmgYxkga"
   },
   "outputs": [],
   "source": [
    "b_ = out1.data.max(0)[1].cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLw_Y8emp9HM"
   },
   "source": [
    "Define the function that maps a 2D image with all the class labels to a segmented image with the specified colored maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkyoipIGZKk9"
   },
   "outputs": [],
   "source": [
    "def decode_segmap(image):\n",
    "    Sky = [128, 128, 128]\n",
    "    Building = [128, 0, 0]\n",
    "    Pole = [192, 192, 128]\n",
    "    Road_marking = [255, 69, 0]\n",
    "    Road = [128, 64, 128]\n",
    "    Pavement = [60, 40, 222]\n",
    "    Tree = [128, 128, 0]\n",
    "    SignSymbol = [192, 128, 128]\n",
    "    Fence = [64, 64, 128]\n",
    "    Car = [64, 0, 128]\n",
    "    Pedestrian = [64, 64, 0]\n",
    "    Bicyclist = [0, 128, 192]\n",
    "\n",
    "    label_colours = np.array([Sky, Building, Pole, Road_marking, Road, \n",
    "                              Pavement, Tree, SignSymbol, Fence, Car, \n",
    "                              Pedestrian, Bicyclist]).astype(np.uint8)\n",
    "    r = np.zeros_like(image).astype(np.uint8)\n",
    "    g = np.zeros_like(image).astype(np.uint8)\n",
    "    b = np.zeros_like(image).astype(np.uint8)\n",
    "    for l in range(0, 12):\n",
    "        r[image == l] = label_colours[l, 0]\n",
    "        g[image == l] = label_colours[l, 1]\n",
    "        b[image == l] = label_colours[l, 2]\n",
    "\n",
    "    rgb = np.zeros((image.shape[0], image.shape[1], 3)).astype(np.uint8)\n",
    "    rgb[:, :, 0] = b\n",
    "    rgb[:, :, 1] = g\n",
    "    rgb[:, :, 2] = r\n",
    "    return rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_B6n9VmqP5g"
   },
   "source": [
    "Decode the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OHyCu5TTaNkv"
   },
   "outputs": [],
   "source": [
    "true_seg = decode_segmap(smg_)\n",
    "pred_seg = decode_segmap(b_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "4G_8C40YXziZ",
    "outputId": "75ed8f50-c135-4eb8-9b5c-1a63e2620991"
   },
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(20, 10))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title('Input Image')\n",
    "plt.axis('off')\n",
    "plt.imshow(tmg_)\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title('Predicted Segmentation')\n",
    "plt.axis('off')\n",
    "plt.imshow(pred_seg)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title('Ground Truth')\n",
    "plt.axis('off')\n",
    "plt.imshow(true_seg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YI65NDYmr7h"
   },
   "source": [
    "## Save the model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WjUZDGfU5F8X"
   },
   "outputs": [],
   "source": [
    "# Save the parameters\n",
    "checkpoint = {\n",
    "    'epochs' : e,\n",
    "    'state_dict' : enet.state_dict()\n",
    "}\n",
    "torch.save(checkpoint, 'ckpt-enet-1.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51UgRcNYmwCc"
   },
   "source": [
    "## Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQVVoXhn5oun"
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(enet, '/content/model.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ENet - Real Time Semantic Segmentation.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "daya_pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
